{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "176634fa",
   "metadata": {},
   "source": [
    "## Spark Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba5a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/03 20:22:54 WARN Utils: Your hostname, cougar resolves to a loopback address: 127.0.1.1; using 192.168.1.150 instead (on interface wlp10s0)\n",
      "25/09/03 20:22:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/pmu/miniconda3/envs/self-drive/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/pmu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/pmu/.ivy2/jars\n",
      "com.clickhouse.spark#clickhouse-spark-runtime-3.5_2.12 added as a dependency\n",
      "com.clickhouse#clickhouse-jdbc added as a dependency\n",
      "com.clickhouse#clickhouse-http-client added as a dependency\n",
      "org.apache.httpcomponents.client5#httpclient5 added as a dependency\n",
      "com.microsoft.azure#synapseml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-626ae638-20df-4b96-b415-a15fc2879d7f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.clickhouse.spark#clickhouse-spark-runtime-3.5_2.12;0.8.0 in central\n",
      "\tfound com.clickhouse#clickhouse-jdbc;0.7.1-patch1 in central\n",
      "\tfound com.clickhouse#clickhouse-client;0.7.1-patch1 in central\n",
      "\tfound com.clickhouse#clickhouse-data;0.7.1-patch1 in central\n",
      "\tfound com.clickhouse#clickhouse-http-client;0.7.1-patch1 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5-h2;5.2 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5;5.2.1 in central\n",
      "\tfound org.apache.httpcomponents.client5#httpclient5;5.3.1 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5;5.2.4 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5-h2;5.2.4 in central\n",
      "\tfound com.microsoft.azure#synapseml_2.12;1.0.8 in central\n",
      "\tfound com.microsoft.azure#synapseml-core_2.12;1.0.8 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.4.1 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound org.scalactic#scalactic_2.12;3.2.14 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.15 in central\n",
      "\tfound io.spray#spray-json_2.12;1.3.5 in central\n",
      "\tfound com.jcraft#jsch;0.1.54 in central\n",
      "\tfound org.apache.httpcomponents#httpmime;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.11 in central\n",
      "\tfound com.linkedin.isolation-forest#isolation-forest_3.4.2_2.12;3.0.4 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.10 in central\n",
      "\tfound org.testng#testng;6.8.8 in central\n",
      "\tfound org.beanshell#bsh;2.0b4 in central\n",
      "\tfound com.beust#jcommander;1.27 in central\n",
      "\tfound org.scalanlp#breeze_2.12;2.1.0 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;2.1.0 in central\n",
      "\tfound org.typelevel#spire_2.12;0.17.0 in central\n",
      "\tfound org.typelevel#spire-macros_2.12;0.17.0 in central\n",
      "\tfound org.typelevel#algebra_2.12;2.0.1 in central\n",
      "\tfound org.typelevel#cats-kernel_2.12;2.1.1 in central\n",
      "\tfound org.typelevel#spire-platform_2.12;0.17.0 in central\n",
      "\tfound org.typelevel#spire-util_2.12;0.17.0 in central\n",
      "\tfound dev.ludovic.netlib#blas;3.0.1 in central\n",
      "\tfound net.sourceforge.f2j#arpack_combined_all;0.1 in central\n",
      "\tfound dev.ludovic.netlib#lapack;3.0.1 in central\n",
      "\tfound dev.ludovic.netlib#arpack;3.0.1 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.wendykierp#JTransforms;3.1 in central\n",
      "\tfound pl.edu.icm#JLargeArrays;1.5 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.7.0 in central\n",
      "\tfound com.microsoft.azure#synapseml-deep-learning_2.12;1.0.8 in central\n",
      "\tfound com.microsoft.azure#synapseml-opencv_2.12;1.0.8 in central\n",
      "\tfound org.openpnp#opencv;3.2.0-1 in central\n",
      "\tfound com.microsoft.azure#onnx-protobuf_2.12;0.9.3 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime_gpu;1.8.1 in central\n",
      "\tfound com.microsoft.azure#synapseml-cognitive_2.12;1.0.8 in central\n",
      "\tfound com.microsoft.cognitiveservices.speech#client-sdk;1.24.1 in central\n",
      "\tfound com.microsoft.azure#synapseml-vw_2.12;1.0.8 in central\n",
      "\tfound com.github.vowpalwabbit#vw-jni;9.3.0 in central\n",
      "\tfound com.microsoft.azure#synapseml-lightgbm_2.12;1.0.8 in central\n",
      "\tfound com.microsoft.ml.lightgbm#lightgbmlib;3.3.510 in central\n",
      ":: resolution report :: resolve 386ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.beust#jcommander;1.27 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.10 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-client;0.7.1-patch1 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-data;0.7.1-patch1 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-http-client;0.7.1-patch1 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-jdbc;0.7.1-patch1 from central in [default]\n",
      "\tcom.clickhouse.spark#clickhouse-spark-runtime-3.5_2.12;0.8.0 from central in [default]\n",
      "\tcom.github.vowpalwabbit#vw-jni;9.3.0 from central in [default]\n",
      "\tcom.github.wendykierp#JTransforms;3.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.54 from central in [default]\n",
      "\tcom.linkedin.isolation-forest#isolation-forest_3.4.2_2.12;3.0.4 from central in [default]\n",
      "\tcom.microsoft.azure#onnx-protobuf_2.12;0.9.3 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-cognitive_2.12;1.0.8 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-core_2.12;1.0.8 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-deep-learning_2.12;1.0.8 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-lightgbm_2.12;1.0.8 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-opencv_2.12;1.0.8 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-vw_2.12;1.0.8 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml_2.12;1.0.8 from central in [default]\n",
      "\tcom.microsoft.cognitiveservices.speech#client-sdk;1.24.1 from central in [default]\n",
      "\tcom.microsoft.ml.lightgbm#lightgbmlib;3.3.510 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime_gpu;1.8.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.11 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tdev.ludovic.netlib#arpack;3.0.1 from central in [default]\n",
      "\tdev.ludovic.netlib#blas;3.0.1 from central in [default]\n",
      "\tdev.ludovic.netlib#lapack;3.0.1 from central in [default]\n",
      "\tio.spray#spray-json_2.12;1.3.5 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\tnet.sourceforge.f2j#arpack_combined_all;0.1 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpmime;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents.client5#httpclient5;5.3.1 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5;5.2.4 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5-h2;5.2.4 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.4.1 from central in [default]\n",
      "\torg.beanshell#bsh;2.0b4 from central in [default]\n",
      "\torg.openpnp#opencv;3.2.0-1 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.15 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.7.0 from central in [default]\n",
      "\torg.scalactic#scalactic_2.12;3.2.14 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;2.1.0 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;2.1.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.testng#testng;6.8.8 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.typelevel#algebra_2.12;2.0.1 from central in [default]\n",
      "\torg.typelevel#cats-kernel_2.12;2.1.1 from central in [default]\n",
      "\torg.typelevel#spire-macros_2.12;0.17.0 from central in [default]\n",
      "\torg.typelevel#spire-platform_2.12;0.17.0 from central in [default]\n",
      "\torg.typelevel#spire-util_2.12;0.17.0 from central in [default]\n",
      "\torg.typelevel#spire_2.12;0.17.0 from central in [default]\n",
      "\tpl.edu.icm#JLargeArrays;1.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.httpcomponents.client5#httpclient5;5.2.1 by [org.apache.httpcomponents.client5#httpclient5;5.3.1] in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5-h2;5.2 by [org.apache.httpcomponents.core5#httpcore5-h2;5.2.4] in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5;5.2.1 by [org.apache.httpcomponents.core5#httpcore5;5.2.4] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\torg.apache.httpcomponents.client5#httpclient5;5.1.3 by [org.apache.httpcomponents.client5#httpclient5;5.3.1] in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.2.0 by [org.scala-lang.modules#scala-collection-compat_2.12;2.7.0] in [default]\n",
      "\torg.apache.commons#commons-math3;3.5 by [org.apache.commons#commons-math3;3.2] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   64  |   0   |   0   |   8   ||   56  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-626ae638-20df-4b96-b415-a15fc2879d7f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 56 already retrieved (0kB/8ms)\n",
      "25/09/03 20:22:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql import Window\n",
    "\n",
    "\n",
    "\n",
    "# ml\n",
    "from pyspark.ml import Pipeline as spk_pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder as spk_OneHotEncoder, StandardScaler as spk_StandardScaler, VectorAssembler as spk_VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler as spk_MinMaxScaler, StringIndexer as spk_StringIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator as spk_RegressionEvaluator\n",
    "\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "\n",
    "\n",
    "import os\n",
    "#https://repo1.maven.org/maven2/com/github/housepower/clickhouse-native-jdbc/2.7.1/clickhouse-native-jdbc-2.7.1.jar\n",
    "# spark connector https://github.com/ClickHouse/spark-clickhouse-connector\n",
    "# https://mvnrepository.com/artifact/com.clickhouse\n",
    "# https://github.com/housepower/ClickHouse-Native-JDBC, For Spark 3.2 and upper, Spark ClickHouse Connector (see upper) is recommended.\n",
    "packages = [\n",
    "    \"com.clickhouse.spark:clickhouse-spark-runtime-3.5_2.12:0.8.0\"\n",
    "    # \"com.github.housepower:clickhouse-spark-runtime-3.4_2.12:0.7.3\"\n",
    "    ,\"com.clickhouse:clickhouse-jdbc:0.7.1-patch1\"\n",
    "    # ,\"com.clickhouse:clickhouse-jdbc:0.6.0-patch5\"\n",
    "    ,\"com.clickhouse:clickhouse-http-client:0.7.1-patch1\"\n",
    "    # ,\"com.clickhouse:clickhouse-http-client:0.6.0-patch5\"\n",
    "    ,\"org.apache.httpcomponents.client5:httpclient5:5.3.1\"\n",
    "    # for jdbc 2.7.1 required java 8/11\n",
    "    # ,\"com.github.housepower:clickhouse-native-jdbc:2.7.1\"\n",
    "    # ,\"ai.catboost:catboost-spark_3.5_2.12:1.2.7\"\n",
    "    ,\"com.microsoft.azure:synapseml_2.12:1.0.8\"\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "ram = 16\n",
    "cpu = 8*3\n",
    "# Define the application name and setup session\n",
    "appName = \"Connect To ClickHouse via PySpark\"\n",
    "spark = (SparkSession.builder\n",
    "         .appName(appName)\n",
    "         # for connetc to external spark master\n",
    "        #  .master('spark://127.0.0.1:7077')\n",
    "         .config(\"spark.jars.packages\", \",\".join(packages))\n",
    "        #  .config(\"spark.sql.catalog.clickhouse\", \"xenon.clickhouse.ClickHouseCatalog\")\n",
    "        #  .config(\"spark.sql.catalog.clickhouse\", \"com.clickhouse.spark.ClickHouseCatalog\")\n",
    "        #  .config(\"spark.sql.catalog.clickhouse.host\", CH_IP)\n",
    "        #  .config(\"spark.sql.catalog.clickhouse.protocol\", \"http\")\n",
    "        #  .config(\"spark.sql.catalog.clickhouse.http_port\", \"8123\")\n",
    "        #  .config(\"spark.sql.catalog.clickhouse.user\", CH_USER)\n",
    "        #  .config(\"spark.sql.catalog.clickhouse.password\", CH_PASS)\n",
    "        #  .config(\"spark.sql.catalog.clickhouse.database\", \"default\")\n",
    "        #  .config(\"spark.spark.clickhouse.write.compression.codec\", \"lz4\")\n",
    "        #  .config(\"spark.clickhouse.read.compression.codec\", \"lz4\")\n",
    "        #  .config(\"spark.clickhouse.write.format\", \"arrow\")\n",
    "         #    .config(\"spark.clickhouse.write.distributed.convertLocal\", \"true\") l\n",
    "         #    .config(\"spark.clickhouse.write.repartitionNum\", \"1\") \n",
    "         #.config(\"spark.clickhouse.write.maxRetry\", \"1000\")\n",
    "         #    .config(\"spark.clickhouse.write.repartitionStrictly\", \"true\") \n",
    "         #    .config(\"spark.clickhouse.write.distributed.useClusterNodes\", \"false\") \n",
    "        #  .config(\"spark.clickhouse.write.batchSize\", \"1000000\")\n",
    "         #.config(\"spark.sql.catalog.clickhouse.socket_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.sql.catalog.clickhouse.connection_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.sql.catalog.clickhouse.query_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.clickhouse.options.socket_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.clickhouse.options.connection_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.clickhouse.options.query_timeout\", \"600000000\")         \n",
    "         .config(\"spark.executor.memory\", f\"{ram}g\")\n",
    "        #  .config(\"spark.executor.cores\", \"5\")\n",
    "         .config(\"spark.driver.maxResultSize\", f\"{ram}g\")\n",
    "         .config(\"spark.driver.memory\", f\"{ram}g\")\n",
    "         .config(\"spark.executor.memoryOverhead\", f\"{ram}g\")\n",
    "        #  .config(\"spark.sql.debug.maxToStringFields\", \"100000\")\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "# LightGBM set config https://microsoft.github.io/SynapseML/docs/Get%20Started/Install%20SynapseML/\n",
    "# spark.conf.set(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "# spark.conf.set(\"spark.jars.excludes\", \",\".join(exclude_packages))\n",
    "# spark.conf.set(\"spark.yarn.user.classpath.first\", \"true\")\n",
    "# spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from catboost_spark import CatBoostRegressor as CatBoostRegressor_spark\n",
    "# from synapse.ml.lightgbm import LightGBMRegressor as LightGBMRegressor_spark\n",
    "\n",
    "\n",
    "# sp_tools = SparkTools(spark, data_path, tmp_path, CH_USER, CH_PASS, CH_IP)\n",
    "\n",
    "# spark.sql(\"use clickhouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d5a4c",
   "metadata": {},
   "source": [
    "## Заказы пеших курьеров\n",
    "\n",
    "У компании по доставке еды есть БД в которой содержится таблица заказов пеших курьеров [couriers_orders.parquet](https://drive.google.com/file/d/1ZJlyPkU8W3qlSJI57qYg1e_PQIxCjwZN/view?usp=sharing)  .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1c2c33e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+--------+--------+-----------+\n",
      "|               date|courier_id|order_id|distance|travel_time|\n",
      "+-------------------+----------+--------+--------+-----------+\n",
      "|2021-07-12 00:00:00|        10|       1|     1.9|      36.17|\n",
      "|2021-07-02 00:00:00|         3|       2|    3.98|      21.34|\n",
      "|2021-04-15 00:00:00|         6|       3|    3.98|      43.33|\n",
      "|2021-07-16 00:00:00|        10|       4|    2.85|      14.01|\n",
      "|2021-06-11 00:00:00|        10|       5|    4.89|      32.09|\n",
      "+-------------------+----------+--------+--------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "couriers_orders = spark.read.parquet(\"data/couriers_orders.parquet\")\n",
    "couriers_orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4b17f2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp_ntz (nullable = true)\n",
      " |-- courier_id: long (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- travel_time: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "couriers_orders.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ccd8fd",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9299ad7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------+--------+-----------+\n",
      "|date|courier_id|order_id|distance|travel_time|\n",
      "+----+----------+--------+--------+-----------+\n",
      "|   0|         0|       0|       0|          0|\n",
      "+----+----------+--------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check null and nan values in each column spark df, nan doesn't work for timestamp dtype\n",
    "def null_nan_counts(dfs: SparkDataFrame):\n",
    "    exprs = []\n",
    "    for c in dfs.columns:\n",
    "        dtype = dict(dfs.dtypes)[c]\n",
    "        if dtype in ['double', 'float']:\n",
    "            expr = F.count(F.when(F.isnull(c) | F.isnan(c), c)).alias(c)\n",
    "        else:\n",
    "            expr = F.count(F.when(F.isnull(c), c)).alias(c)\n",
    "        exprs.append(expr)\n",
    "    return dfs.select(*exprs).show()\n",
    "    \n",
    "\n",
    "null_nan_counts(couriers_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66c10368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|          distance|       travel_time|\n",
      "+-------+------------------+------------------+\n",
      "|  count|              1666|              1666|\n",
      "|   mean|2.7334933973589397| 34.81731692677075|\n",
      "| stddev|1.2954651948730809|14.475623110887007|\n",
      "|    min|               0.5|             10.01|\n",
      "|    max|               5.0|             59.97|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "couriers_orders.select(F.col(\"distance\"), F.col(\"travel_time\")).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93864b2",
   "metadata": {},
   "source": [
    "### Вопрос №1.1:\n",
    "\n",
    "В конце каждого месяца компания выдает премию для своих курьеров, средняя скорость доставки за прошедший месяц которых больше средней скорости среди всех курьеров. Сколько курьеров получили премию за июнь 2021 года.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d2270fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+--------+--------+-----------+------------------+\n",
      "|               date|courier_id|order_id|distance|travel_time|       order_speed|\n",
      "+-------------------+----------+--------+--------+-----------+------------------+\n",
      "|2021-06-11 00:00:00|        10|       5|    4.89|      32.09| 9.143035213462136|\n",
      "|2021-06-14 00:00:00|         4|       9|    4.13|      29.34| 8.445807770961146|\n",
      "|2021-06-27 00:00:00|         8|      10|    1.04|      12.56| 4.968152866242038|\n",
      "|2021-06-27 00:00:00|         1|      19|    1.85|      13.56| 8.185840707964601|\n",
      "|2021-06-28 00:00:00|         2|      25|    4.02|      12.43|19.404666130329844|\n",
      "+-------------------+----------+--------+--------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# travel time in min and distance in km, add speed column to couriers_orders km/h and filter only june orders\n",
    "couriers_orders_june = couriers_orders.withColumn(\"order_speed\", F.expr(\"distance / (travel_time / 60)\")) \\\n",
    "    .filter((F.col('date') >= F.lit('2021-06-01')) & (F.col('date') <= F.lit('2021-06-30')))\n",
    "\n",
    "couriers_orders_june.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a20d9ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+\n",
      "|courier_id|mean_speed_by_courier|\n",
      "+----------+---------------------+\n",
      "|         6|    6.791953132084712|\n",
      "|         1|   6.7493254093092405|\n",
      "|        10|     6.58047952209051|\n",
      "|         8|    6.826371958342846|\n",
      "|         2|    6.528423151187261|\n",
      "|         4|    7.692341132350769|\n",
      "+----------+---------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 6)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add column with mean speed by courier_id\n",
    "\n",
    "# compute mean speed per courier\n",
    "mean_speed_courier = (\n",
    "    couriers_orders_june.groupBy(\"courier_id\")\n",
    "    .agg(F.mean(\"order_speed\").alias(\"mean_speed_by_courier\"))\n",
    ")\n",
    "\n",
    "# compute overall mean (as DataFrame, not collected scalar)\n",
    "overall_mean_df = mean_speed_courier.agg(\n",
    "    F.mean(\"mean_speed_by_courier\").alias(\"overall_mean_speed\")\n",
    ")\n",
    "\n",
    "# Filter couriers whose mean speed is greater than overall mean\n",
    "couriers_bonus = mean_speed_courier.filter(F.col(\"mean_speed_by_courier\") > overall_mean_df.collect()[0][\"overall_mean_speed\"])\n",
    "couriers_bonus.show(), couriers_bonus.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe59468",
   "metadata": {},
   "source": [
    "### Вопрос №1.2:\n",
    "используйте данные из предыдущего вопроса №1.1\n",
    "\n",
    "Компания хочет понять, насколько равномерно курьеры работают в течение месяца. Для этого нужно найти ID курьера с наибольшей разницей между максимальной и минимальной средней дневной скоростью в июне 2021 года."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "54e82b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+------------------+\n",
      "|               date|courier_id|  daily_mean_speed|\n",
      "+-------------------+----------+------------------+\n",
      "|2021-06-21 00:00:00|         6| 8.125091054426372|\n",
      "|2021-06-25 00:00:00|         9| 8.552623561272851|\n",
      "|2021-06-25 00:00:00|         8| 5.324176574701887|\n",
      "|2021-06-30 00:00:00|         8|1.7039153083515375|\n",
      "|2021-06-03 00:00:00|         6| 1.403067638923812|\n",
      "+-------------------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_mean_speed_june = couriers_orders_june.groupBy(\"date\", \"courier_id\").agg(F.mean(\"order_speed\").alias(\"daily_mean_speed\"))\n",
    "daily_mean_speed_june.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cecf2149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------+--------------------------+------------------+\n",
      "|courier_id|max_daily_speed_by_courier|min_daily_speed_by_courier|        speed_diff|\n",
      "+----------+--------------------------+--------------------------+------------------+\n",
      "|         7|         11.33508936970837|        1.0535557506584723|10.281533619049899|\n",
      "|         6|         25.49718574108818|         1.403067638923812| 24.09411810216437|\n",
      "|         9|        13.315508021390373|        0.7547857793983591|12.560722241992014|\n",
      "|         5|        17.651376146788987|         1.104111823559212|16.547264323229776|\n",
      "|         1|        25.342333654773388|        1.6655313351498635|23.676802319623523|\n",
      "+----------+--------------------------+--------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+------------------+\n",
      "|courier_id|        speed_diff|\n",
      "+----------+------------------+\n",
      "|         4|24.802606662802628|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courier_speed_stats = daily_mean_speed_june.groupBy(\"courier_id\")\\\n",
    "    .agg(F.max(\"daily_mean_speed\").alias(\"max_daily_speed_by_courier\"))\\\n",
    "    .join(\n",
    "        daily_mean_speed_june.groupBy(\"courier_id\")\\\n",
    "            .agg(F.min(\"daily_mean_speed\").alias(\"min_daily_speed_by_courier\")),\n",
    "        \"courier_id\"\n",
    "    )\\\n",
    "    .withColumn(\"speed_diff\", F.col(\"max_daily_speed_by_courier\") - F.col(\"min_daily_speed_by_courier\"))\n",
    "\n",
    "\n",
    "courier_speed_stats.show(5)\n",
    "\n",
    "courier_speed_stats\\\n",
    "    .filter(F.col(\"speed_diff\") == courier_speed_stats.select(F.max(\"speed_diff\")).first()[0])\\\n",
    "    .select(\"courier_id\", \"speed_diff\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311bfa53",
   "metadata": {},
   "source": [
    "## Покупки клиентов\n",
    "\n",
    "У нас есть данные о покупках клиентов [purchases.parquet](https://drive.google.com/file/d/1LWqMEDVX5M3iEGPv7zZh2NnlqSMvOcom/view?usp=sharing). Проанализируйте интервалы времени между последовательными покупками для каждого клиента в наборе данных о покупках - напишите код для вычисления разницы в днях между текущей покупкой и предыдущей покупкой каждого клиента. Отобразите результат в новом столбце days_between_purchases.\n",
    "\n",
    "### Вопрос №2.1:\n",
    "\n",
    "Какое количество NaN в столбце days_between_purchases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5faabd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|customer_id|      purchase_date|\n",
      "+-----------+-------------------+\n",
      "|          2|2021-01-01 00:00:00|\n",
      "|          7|2021-01-01 00:00:00|\n",
      "|          7|2021-01-01 00:00:00|\n",
      "|         11|2021-01-01 00:00:00|\n",
      "|         21|2021-01-01 00:00:00|\n",
      "+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchases = spark.read.parquet(\"data/purchases.parquet\")\n",
    "purchases.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "defe39cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----------------------+\n",
      "|customer_id|      purchase_date|days_between_purchases|\n",
      "+-----------+-------------------+----------------------+\n",
      "|          1|2021-01-14 00:00:00|                  NULL|\n",
      "|          1|2021-01-18 00:00:00|                     4|\n",
      "|          1|2021-01-28 00:00:00|                    10|\n",
      "|          1|2021-02-05 00:00:00|                     8|\n",
      "|          1|2021-02-06 00:00:00|                     1|\n",
      "+-----------+-------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchases_days_bt = purchases.withColumn('days_between_purchases', F.datediff(\n",
    "    F.col('purchase_date'), F.lag('purchase_date')\\\n",
    "        .over(Window.partitionBy('customer_id').orderBy('purchase_date'))))\n",
    "\n",
    "purchases_days_bt.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e5a88cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find count of rows where days_between_purchases is null\n",
    "purchases_days_bt.filter(F.col('days_between_purchases').isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fbd045",
   "metadata": {},
   "source": [
    "### Вопрос №2.2 :\n",
    "(используйте данные из предыдущего вопроса №2.1)\n",
    "\n",
    "У какого количества уникальных клиентов разница между текущей покупкой и предыдущей покупкой равна 20-ти дням?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5c28d198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchases_days_bt.filter(F.col('days_between_purchases') == 20).groupBy('customer_id').count().count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-drive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
