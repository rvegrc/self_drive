{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pytz\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import clickhouse_connect\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# turn off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# set all columns to be displayed\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# import tools\n",
    "\n",
    "from tools import pd_tools, spark_tools, db_tools\n",
    "\n",
    "\n",
    "root_path = \".\"\n",
    "tmp_path = f'{root_path}/tmp'\n",
    "data_path = f'{root_path}/data/self-drive'\n",
    "train_data_path = f'{data_path}/train_data'\n",
    "test_data_path = f'{data_path}/test_data'\n",
    "tmp_data_path=f'{data_path}/tmp_data'\n",
    "\n",
    "\n",
    "your_mlflow_tracking_uri = f'{root_path}/mlruns' # for docker mlflow server\n",
    "# your_mlflow_tracking_uri = \"http://127.0.0.1:5000\" # for local mlflow server\n",
    "# your_mlflow_tracking_uri = MLFLOW_TRACKING_URI # for remote mlflow server\n",
    "mlflow.set_tracking_uri(your_mlflow_tracking_uri)\n",
    "\n",
    "# constants\n",
    "CH_USER = os.getenv(\"CH_USER\")\n",
    "CH_PASS = os.getenv(\"CH_PASS\")\n",
    "CH_IP = os.getenv('CH_IP')\n",
    "RAND_ST = 354\n",
    "# Define the timezone\n",
    "EXP_TIMEZONE = pytz.timezone('Etc/GMT-3')\n",
    "# MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI')\n",
    "\n",
    "ch_client = clickhouse_connect.get_client(host=CH_IP, port=8123, username=CH_USER, password=CH_PASS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.clickhouse.spark#clickhouse-spark-runtime-3.5_2.12 added as a dependency\n",
      "com.clickhouse#clickhouse-jdbc added as a dependency\n",
      "com.clickhouse#clickhouse-http-client added as a dependency\n",
      "org.apache.httpcomponents.client5#httpclient5 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a04aa209-5337-4fb5-bec1-b89bbbccaf43;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.clickhouse.spark#clickhouse-spark-runtime-3.5_2.12;0.8.0 in central\n",
      "\tfound com.clickhouse#clickhouse-jdbc;0.7.1-patch1 in central\n",
      "\tfound com.clickhouse#clickhouse-client;0.7.1-patch1 in central\n",
      "\tfound com.clickhouse#clickhouse-data;0.7.1-patch1 in central\n",
      "\tfound com.clickhouse#clickhouse-http-client;0.7.1-patch1 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5-h2;5.2 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5;5.2.1 in central\n",
      "\tfound org.apache.httpcomponents.client5#httpclient5;5.3.1 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5;5.2.4 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5-h2;5.2.4 in central\n",
      ":: resolution report :: resolve 503ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.clickhouse#clickhouse-client;0.7.1-patch1 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-data;0.7.1-patch1 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-http-client;0.7.1-patch1 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-jdbc;0.7.1-patch1 from central in [default]\n",
      "\tcom.clickhouse.spark#clickhouse-spark-runtime-3.5_2.12;0.8.0 from central in [default]\n",
      "\torg.apache.httpcomponents.client5#httpclient5;5.3.1 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5;5.2.4 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5-h2;5.2.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.httpcomponents.client5#httpclient5;5.2.1 by [org.apache.httpcomponents.client5#httpclient5;5.3.1] in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5-h2;5.2 by [org.apache.httpcomponents.core5#httpcore5-h2;5.2.4] in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5;5.2.1 by [org.apache.httpcomponents.core5#httpcore5;5.2.4] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   4   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a04aa209-5337-4fb5-bec1-b89bbbccaf43\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/8ms)\n",
      "25/01/06 13:49:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql import Window\n",
    "\n",
    "\n",
    "\n",
    "# ml\n",
    "from pyspark.ml import Pipeline as spk_pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder as spk_OneHotEncoder, StandardScaler as spk_StandardScaler, VectorAssembler as spk_VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler as spk_MinMaxScaler, StringIndexer as spk_StringIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator as spk_RegressionEvaluator\n",
    "\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "#https://repo1.maven.org/maven2/com/github/housepower/clickhouse-native-jdbc/2.7.1/clickhouse-native-jdbc-2.7.1.jar\n",
    "# spark connector https://github.com/ClickHouse/spark-clickhouse-connector\n",
    "# https://mvnrepository.com/artifact/com.clickhouse\n",
    "# https://github.com/housepower/ClickHouse-Native-JDBC, For Spark 3.2 and upper, Spark ClickHouse Connector (see upper) is recommended.\n",
    "packages = [\n",
    "    \"com.clickhouse.spark:clickhouse-spark-runtime-3.5_2.12:0.8.0\"\n",
    "    # \"com.github.housepower:clickhouse-spark-runtime-3.4_2.12:0.7.3\"\n",
    "    ,\"com.clickhouse:clickhouse-jdbc:0.7.1-patch1\"\n",
    "    # ,\"com.clickhouse:clickhouse-jdbc:0.6.0-patch5\"\n",
    "    ,\"com.clickhouse:clickhouse-http-client:0.7.1-patch1\"\n",
    "    # ,\"com.clickhouse:clickhouse-http-client:0.6.0-patch5\"\n",
    "    ,\"org.apache.httpcomponents.client5:httpclient5:5.3.1\"\n",
    "    # for jdbc 2.7.1 required java 8/11\n",
    "    # ,\"com.github.housepower:clickhouse-native-jdbc:2.7.1\"\n",
    "    # ,\"ai.catboost:catboost-spark_3.5_2.12:1.2.7\"\n",
    "    # ,\"com.microsoft.azure:synapseml_2.12:1.0.8\"\n",
    "\n",
    "]\n",
    "\n",
    "# exclude_packages = [\n",
    "#     \"org.scala-lang:scala-reflect\"\n",
    "#     ,\"org.apache.spark:spark-tags_2.12\"\n",
    "#     ,\"org.scalactic:scalactic_2.12\"\n",
    "#     ,\"org.scalatest:scalatest_2.12\"\n",
    "#     ,\"com.fasterxml.jackson.core:jackson-databind\"\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "ram = 10\n",
    "cpu = 22*3\n",
    "# Define the application name and setup session\n",
    "appName = \"Connect To ClickHouse via PySpark\"\n",
    "spark = (SparkSession.builder\n",
    "         .appName(appName)\n",
    "         .config(\"spark.jars.packages\", \",\".join(packages))\n",
    "        #  .config(\"spark.sql.catalog.clickhouse\", \"xenon.clickhouse.ClickHouseCatalog\")\n",
    "         .config(\"spark.sql.catalog.clickhouse\", \"com.clickhouse.spark.ClickHouseCatalog\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.host\", CH_IP)\n",
    "         .config(\"spark.sql.catalog.clickhouse.protocol\", \"http\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.http_port\", \"8123\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.user\", CH_USER)\n",
    "         .config(\"spark.sql.catalog.clickhouse.password\", CH_PASS)\n",
    "         .config(\"spark.sql.catalog.clickhouse.database\", \"default\")\n",
    "        #  .config(\"spark.spark.clickhouse.write.compression.codec\", \"lz4\")\n",
    "        #  .config(\"spark.clickhouse.read.compression.codec\", \"lz4\")\n",
    "        #  .config(\"spark.clickhouse.write.format\", \"arrow\")\n",
    "         #    .config(\"spark.clickhouse.write.distributed.convertLocal\", \"true\") l\n",
    "         #    .config(\"spark.clickhouse.write.repartitionNum\", \"1\") \n",
    "         #.config(\"spark.clickhouse.write.maxRetry\", \"1000\")\n",
    "         #    .config(\"spark.clickhouse.write.repartitionStrictly\", \"true\") \n",
    "         #    .config(\"spark.clickhouse.write.distributed.useClusterNodes\", \"false\") \n",
    "        #  .config(\"spark.clickhouse.write.batchSize\", \"1000000\")\n",
    "         #.config(\"spark.sql.catalog.clickhouse.socket_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.sql.catalog.clickhouse.connection_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.sql.catalog.clickhouse.query_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.clickhouse.options.socket_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.clickhouse.options.connection_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.clickhouse.options.query_timeout\", \"600000000\")         \n",
    "         .config(\"spark.executor.memory\", f\"{ram}g\")\n",
    "        #  .config(\"spark.executor.cores\", \"5\")\n",
    "         .config(\"spark.driver.maxResultSize\", f\"{ram}g\")\n",
    "         .config(\"spark.driver.memory\", f\"{ram}g\")\n",
    "         .config(\"spark.executor.memoryOverhead\", f\"{ram}g\")\n",
    "        #  .config(\"spark.sql.debug.maxToStringFields\", \"100000\")\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "# LightGBM set config https://microsoft.github.io/SynapseML/docs/Get%20Started/Install%20SynapseML/\n",
    "# spark.conf.set(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "# spark.conf.set(\"spark.jars.excludes\", \",\".join(exclude_packages))\n",
    "# spark.conf.set(\"spark.yarn.user.classpath.first\", \"true\")\n",
    "# spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "\n",
    "#SedonaRegistrator.registerAll(spark)\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse\", \"xenon.clickhouse.ClickHouseCatalog\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.host\", \"127.0.0.1\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.protocol\", \"http\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.http_port\", \"8123\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.user\", \"default\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.password\", \"\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.database\", \"default\")\n",
    "\n",
    "\n",
    "\n",
    "# from catboost_spark import CatBoostRegressor as CatBoostRegressor_spark\n",
    "# from synapse.ml.lightgbm import LightGBMRegressor as LightGBMRegressor_spark\n",
    "\n",
    "\n",
    "spark.sql(\"use clickhouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read folder names in path\n",
    "def read_names(path: str):\n",
    "    '''Read folder names or file names in the path'''\n",
    "    return os.listdir(path)\n",
    "\n",
    "train_ids = pd.Series(read_names(train_data_path)).apply(int).sort_values().reset_index(drop=True)\n",
    "test_ids = pd.Series(read_names(test_data_path)).apply(int).sort_values().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tires_to_columns_date(metadata:pd.DataFrame):\n",
    "    '''Change tires column to front and rear columns and \n",
    "    convert ride_date to datetime and add year, month, day columns'''\n",
    "    metadata['front_tire'] = metadata['tires'][0]\n",
    "    metadata['rear_tire'] = metadata['tires'][1]\n",
    "    metadata = metadata.drop(columns=['tires']).reset_index(drop=True).loc[:0]\n",
    "    # convert ride_date to datetime and add year, month, day columns\n",
    "    metadata['ride_date'] = pd.to_datetime(metadata['ride_date'])\n",
    "    metadata['ride_year'] = metadata['ride_date'].dt.year\n",
    "    metadata['ride_month'] = metadata['ride_date'].dt.month\n",
    "    metadata['ride_day'] = metadata['ride_date'].dt.day\n",
    "    metadata = metadata.drop(columns=['ride_date'])\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n"
     ]
    }
   ],
   "source": [
    "ctl = []\n",
    "lcz = []\n",
    "mtd = []\n",
    "for id in test_ids[:]:\n",
    "    control = pd.read_csv(f'{test_data_path}/{id}/control.csv')\n",
    "    localization = pd.read_csv(f'{test_data_path}/{id}/localization.csv')\n",
    "    metadata = tires_to_columns_date(pd.read_json(f'{test_data_path}/{id}/metadata.json'))\n",
    "    control['id'] = id\n",
    "    localization['id'] = id\n",
    "    metadata['id'] = id\n",
    "    ctl.append(control)\n",
    "    lcz.append(localization)\n",
    "    mtd.append(metadata)\n",
    "    if id % 1000 == 0:\n",
    "        print(id)\n",
    "\n",
    "pd.concat(ctl).to_parquet(f'{tmp_data_path}/test_control.parquet', index=False)\n",
    "pd.concat(lcz).to_parquet(f'{tmp_data_path}/test_localization.parquet', index=False)\n",
    "pd.concat(mtd).to_parquet(f'{tmp_data_path}/test_metadata.parquet', index=False)\n",
    "# ch_client.insert_df(f'{db_name}.{table_name}', pd.concat(ctl))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n"
     ]
    }
   ],
   "source": [
    "ctl = []\n",
    "lcz = []\n",
    "mtd = []\n",
    "for id in train_ids[:]:\n",
    "    control = pd.read_csv(f'{train_data_path}/{id}/control.csv')\n",
    "    localization = pd.read_csv(f'{train_data_path}/{id}/localization.csv')\n",
    "    metadata = tires_to_columns_date(pd.read_json(f'{train_data_path}/{id}/metadata.json'))\n",
    "    control['id'] = id\n",
    "    localization['id'] = id\n",
    "    metadata['id'] = id\n",
    "    ctl.append(control)\n",
    "    lcz.append(localization)\n",
    "    mtd.append(metadata)\n",
    "    if id % 1000 == 0:\n",
    "        print(id)\n",
    "\n",
    "pd.concat(ctl).to_parquet(f'{tmp_data_path}/train_control.parquet', index=False)\n",
    "pd.concat(lcz).to_parquet(f'{tmp_data_path}/train_localization.parquet', index=False)\n",
    "pd.concat(mtd).to_parquet(f'{tmp_data_path}/train_metadata.parquet', index=False)\n",
    "# ch_client.insert_df(f'{db_name}.{table_name}', pd.concat(ctl))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to ycup.test_control in ClickHouse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to ycup.test_localization in ClickHouse.\n",
      "Data successfully written to ycup.test_metadata in ClickHouse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to ycup.train_control in ClickHouse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to ycup.train_localization in ClickHouse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to ycup.train_metadata in ClickHouse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "db_name = 'ycup'\n",
    "tables = ['test_control', 'test_localization', 'test_metadata', 'train_control', 'train_localization', 'train_metadata']\n",
    "\n",
    "# upload data with spark\n",
    "for table in tables[:]:\n",
    "    try:\n",
    "    # Read the Parquet file into a DataFrame\n",
    "        dfs = spark.read.parquet(f'{tmp_data_path}/{table}.parquet')\n",
    "        (\n",
    "            dfs.write.format(\"jdbc\")\n",
    "            .option(\"url\", f\"jdbc:clickhouse://{CH_IP}\")\n",
    "            .option(\"driver\", \"com.clickhouse.jdbc.ClickHouseDriver\")\n",
    "            .option(\"dbtable\", f\"{db_name}.{table}\") # table name\n",
    "            .option(\"user\", CH_USER)\n",
    "            .option(\"password\", CH_PASS)\n",
    "            .option(\"isolationLevel\", \"NONE\")\n",
    "            .mode(\"append\")\n",
    "            .save()\n",
    "        )\n",
    "\n",
    "        \n",
    "        print(f\"Data successfully written to {db_name}.{table} in ClickHouse.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+----------+\n",
      "|  id|  stamp_ns|acceleration_level|  steering|\n",
      "+----+----------+------------------+----------+\n",
      "|2097|3959462781|              1171|-5.5657053|\n",
      "|2097|3999340820|              1187|-5.8483963|\n",
      "|2097|4039323147|              1198|-6.1243787|\n",
      "|2097|4079194872|              1201|-6.3756185|\n",
      "|2097|4118969941|              1200| -6.614259|\n",
      "+----+----------+------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select * from {db_name}.{tables[0]}\").show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
