{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pytz\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import clickhouse_connect\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# turn off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# set all columns to be displayed\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# import tools\n",
    "\n",
    "from tools import pd_tools, spark_tools, db_tools\n",
    "\n",
    "\n",
    "root_path = \".\"\n",
    "tmp_path = f'{root_path}/tmp'\n",
    "data_path = f'{root_path}/data/self-drive'\n",
    "train_data_path = f'{data_path}/train_data'\n",
    "test_data_path = f'{data_path}/test_data'\n",
    "tmp_data_path=f'{data_path}/tmp_data'\n",
    "\n",
    "\n",
    "your_mlflow_tracking_uri = f'{root_path}/mlruns' # for docker mlflow server\n",
    "# your_mlflow_tracking_uri = \"http://127.0.0.1:5000\" # for local mlflow server\n",
    "# your_mlflow_tracking_uri = MLFLOW_TRACKING_URI # for remote mlflow server\n",
    "mlflow.set_tracking_uri(your_mlflow_tracking_uri)\n",
    "\n",
    "# constants\n",
    "CH_USER = os.getenv(\"CH_USER\")\n",
    "CH_PASS = os.getenv(\"CH_PASS\")\n",
    "CH_IP = os.getenv('CH_IP')\n",
    "RAND_ST = 354\n",
    "# Define the timezone\n",
    "EXP_TIMEZONE = pytz.timezone('Etc/GMT-3')\n",
    "# MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI')\n",
    "\n",
    "ch_client = clickhouse_connect.get_client(host=CH_IP, port=8123, username=CH_USER, password=CH_PASS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloading https://repo1.maven.org/maven2/ai/catboost/catboost-spark_3.5_2.12/1.2.7/catboost-spark_3.5_2.12-1.2.7.jar ...\n",
      "downloading https://repo1.maven.org/maven2/ai/catboost/catboost-spark_3.5_2.12/1.2.7/catboost-spark_3.5_2.12-1.2.7.jar ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.clickhouse.spark#clickhouse-spark-runtime-3.5_2.12 added as a dependency\n",
      "com.clickhouse#clickhouse-jdbc added as a dependency\n",
      "com.clickhouse#clickhouse-http-client added as a dependency\n",
      "org.apache.httpcomponents.client5#httpclient5 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ff79094c-1987-49db-9ba9-9f30359ee299;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.clickhouse.spark#clickhouse-spark-runtime-3.5_2.12;0.8.0 in central\n",
      "\tfound com.clickhouse#clickhouse-jdbc;0.7.1-patch1 in central\n",
      "\tfound com.clickhouse#clickhouse-client;0.7.1-patch1 in central\n",
      "\tfound com.clickhouse#clickhouse-data;0.7.1-patch1 in central\n",
      "\tfound com.clickhouse#clickhouse-http-client;0.7.1-patch1 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5-h2;5.2 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5;5.2.1 in central\n",
      "\tfound org.apache.httpcomponents.client5#httpclient5;5.3.1 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5;5.2.4 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5-h2;5.2.4 in central\n",
      "downloading https://repo1.maven.org/maven2/com/clickhouse/clickhouse-client/0.7.1-patch1/clickhouse-client-0.7.1-patch1.jar ...\n",
      "\t[SUCCESSFUL ] com.clickhouse#clickhouse-client;0.7.1-patch1!clickhouse-client.jar (1258ms)\n",
      "downloading https://repo1.maven.org/maven2/com/clickhouse/clickhouse-data/0.7.1-patch1/clickhouse-data-0.7.1-patch1.jar ...\n",
      "\t[SUCCESSFUL ] com.clickhouse#clickhouse-data;0.7.1-patch1!clickhouse-data.jar (670ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (188ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/core5/httpcore5/5.2.4/httpcore5-5.2.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents.core5#httpcore5;5.2.4!httpcore5.jar (998ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/core5/httpcore5-h2/5.2.4/httpcore5-h2-5.2.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents.core5#httpcore5-h2;5.2.4!httpcore5-h2.jar (674ms)\n",
      ":: resolution report :: resolve 1010ms :: artifacts dl 3819ms\n",
      "\t:: modules in use:\n",
      "\tcom.clickhouse#clickhouse-client;0.7.1-patch1 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-data;0.7.1-patch1 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-http-client;0.7.1-patch1 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-jdbc;0.7.1-patch1 from central in [default]\n",
      "\tcom.clickhouse.spark#clickhouse-spark-runtime-3.5_2.12;0.8.0 from central in [default]\n",
      "\torg.apache.httpcomponents.client5#httpclient5;5.3.1 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5;5.2.4 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5-h2;5.2.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.httpcomponents.client5#httpclient5;5.2.1 by [org.apache.httpcomponents.client5#httpclient5;5.3.1] in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5-h2;5.2 by [org.apache.httpcomponents.core5#httpcore5-h2;5.2.4] in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5;5.2.1 by [org.apache.httpcomponents.core5#httpcore5;5.2.4] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   4   ||   9   |   5   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ff79094c-1987-49db-9ba9-9f30359ee299\n",
      "\tconfs: [default]\n",
      "\t9 artifacts copied, 0 already retrieved (5177kB/22ms)\n",
      "25/01/06 13:32:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql import Window\n",
    "\n",
    "\n",
    "\n",
    "# ml\n",
    "from pyspark.ml import Pipeline as spk_pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder as spk_OneHotEncoder, StandardScaler as spk_StandardScaler, VectorAssembler as spk_VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler as spk_MinMaxScaler, StringIndexer as spk_StringIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator as spk_RegressionEvaluator\n",
    "\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "#https://repo1.maven.org/maven2/com/github/housepower/clickhouse-native-jdbc/2.7.1/clickhouse-native-jdbc-2.7.1.jar\n",
    "# spark connector https://github.com/ClickHouse/spark-clickhouse-connector\n",
    "# https://mvnrepository.com/artifact/com.clickhouse\n",
    "# https://github.com/housepower/ClickHouse-Native-JDBC, For Spark 3.2 and upper, Spark ClickHouse Connector (see upper) is recommended.\n",
    "packages = [\n",
    "    \"com.clickhouse.spark:clickhouse-spark-runtime-3.5_2.12:0.8.0\"\n",
    "    # \"com.github.housepower:clickhouse-spark-runtime-3.4_2.12:0.7.3\"\n",
    "    ,\"com.clickhouse:clickhouse-jdbc:0.7.1-patch1\"\n",
    "    # ,\"com.clickhouse:clickhouse-jdbc:0.6.0-patch5\"\n",
    "    ,\"com.clickhouse:clickhouse-http-client:0.7.1-patch1\"\n",
    "    # ,\"com.clickhouse:clickhouse-http-client:0.6.0-patch5\"\n",
    "    ,\"org.apache.httpcomponents.client5:httpclient5:5.3.1\"\n",
    "    # for jdbc 2.7.1 required java 8/11\n",
    "    # ,\"com.github.housepower:clickhouse-native-jdbc:2.7.1\"\n",
    "    # ,\"ai.catboost:catboost-spark_3.5_2.12:1.2.7\"\n",
    "    # ,\"com.microsoft.azure:synapseml_2.12:1.0.8\"\n",
    "\n",
    "]\n",
    "\n",
    "# exclude_packages = [\n",
    "#     \"org.scala-lang:scala-reflect\"\n",
    "#     ,\"org.apache.spark:spark-tags_2.12\"\n",
    "#     ,\"org.scalactic:scalactic_2.12\"\n",
    "#     ,\"org.scalatest:scalatest_2.12\"\n",
    "#     ,\"com.fasterxml.jackson.core:jackson-databind\"\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "ram = 60\n",
    "cpu = 22*3\n",
    "# Define the application name and setup session\n",
    "appName = \"Connect To ClickHouse via PySpark\"\n",
    "spark = (SparkSession.builder\n",
    "         .appName(appName)\n",
    "         .config(\"spark.jars.packages\", \",\".join(packages))\n",
    "        #  .config(\"spark.sql.catalog.clickhouse\", \"xenon.clickhouse.ClickHouseCatalog\")\n",
    "         .config(\"spark.sql.catalog.clickhouse\", \"com.clickhouse.spark.ClickHouseCatalog\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.host\", CH_IP)\n",
    "         .config(\"spark.sql.catalog.clickhouse.protocol\", \"http\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.http_port\", \"8123\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.user\", CH_USER)\n",
    "         .config(\"spark.sql.catalog.clickhouse.password\", CH_PASS)\n",
    "         .config(\"spark.sql.catalog.clickhouse.database\", \"default\")\n",
    "        #  .config(\"spark.spark.clickhouse.write.compression.codec\", \"lz4\")\n",
    "        #  .config(\"spark.clickhouse.read.compression.codec\", \"lz4\")\n",
    "        #  .config(\"spark.clickhouse.write.format\", \"arrow\")\n",
    "         #    .config(\"spark.clickhouse.write.distributed.convertLocal\", \"true\") l\n",
    "         #    .config(\"spark.clickhouse.write.repartitionNum\", \"1\") \n",
    "         #.config(\"spark.clickhouse.write.maxRetry\", \"1000\")\n",
    "         #    .config(\"spark.clickhouse.write.repartitionStrictly\", \"true\") \n",
    "         #    .config(\"spark.clickhouse.write.distributed.useClusterNodes\", \"false\") \n",
    "        #  .config(\"spark.clickhouse.write.batchSize\", \"1000000\")\n",
    "         #.config(\"spark.sql.catalog.clickhouse.socket_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.sql.catalog.clickhouse.connection_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.sql.catalog.clickhouse.query_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.clickhouse.options.socket_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.clickhouse.options.connection_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.clickhouse.options.query_timeout\", \"600000000\")         \n",
    "         .config(\"spark.executor.memory\", f\"{ram}g\")\n",
    "        #  .config(\"spark.executor.cores\", \"5\")\n",
    "         .config(\"spark.driver.maxResultSize\", f\"{ram}g\")\n",
    "         .config(\"spark.driver.memory\", f\"{ram}g\")\n",
    "         .config(\"spark.executor.memoryOverhead\", f\"{ram}g\")\n",
    "        #  .config(\"spark.sql.debug.maxToStringFields\", \"100000\")\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "# LightGBM set config https://microsoft.github.io/SynapseML/docs/Get%20Started/Install%20SynapseML/\n",
    "# spark.conf.set(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "# spark.conf.set(\"spark.jars.excludes\", \",\".join(exclude_packages))\n",
    "# spark.conf.set(\"spark.yarn.user.classpath.first\", \"true\")\n",
    "# spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "\n",
    "#SedonaRegistrator.registerAll(spark)\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse\", \"xenon.clickhouse.ClickHouseCatalog\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.host\", \"127.0.0.1\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.protocol\", \"http\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.http_port\", \"8123\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.user\", \"default\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.password\", \"\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.database\", \"default\")\n",
    "\n",
    "\n",
    "\n",
    "# from catboost_spark import CatBoostRegressor as CatBoostRegressor_spark\n",
    "# from synapse.ml.lightgbm import LightGBMRegressor as LightGBMRegressor_spark\n",
    "\n",
    "\n",
    "spark.sql(\"use clickhouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read folder names in path\n",
    "def read_names(path: str):\n",
    "    '''Read folder names or file names in the path'''\n",
    "    return os.listdir(path)\n",
    "\n",
    "train_ids = pd.Series(read_names(train_data_path)).apply(int).sort_values().reset_index(drop=True)\n",
    "test_ids = pd.Series(read_names(test_data_path)).apply(int).sort_values().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tires_to_columns_date(metadata:pd.DataFrame):\n",
    "    '''Change tires column to front and rear columns and \n",
    "    convert ride_date to datetime and add year, month, day columns'''\n",
    "    metadata['front_tire'] = metadata['tires'][0]\n",
    "    metadata['rear_tire'] = metadata['tires'][1]\n",
    "    metadata = metadata.drop(columns=['tires']).reset_index(drop=True).loc[:0]\n",
    "    # convert ride_date to datetime and add year, month, day columns\n",
    "    metadata['ride_date'] = pd.to_datetime(metadata['ride_date'])\n",
    "    metadata['ride_year'] = metadata['ride_date'].dt.year\n",
    "    metadata['ride_month'] = metadata['ride_date'].dt.month\n",
    "    metadata['ride_day'] = metadata['ride_date'].dt.day\n",
    "    metadata = metadata.drop(columns=['ride_date'])\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n"
     ]
    }
   ],
   "source": [
    "ctl = []\n",
    "lcz = []\n",
    "mtd = []\n",
    "for id in test_ids[:]:\n",
    "    control = pd.read_csv(f'{test_data_path}/{id}/control.csv')\n",
    "    localization = pd.read_csv(f'{test_data_path}/{id}/localization.csv')\n",
    "    metadata = tires_to_columns_date(pd.read_json(f'{test_data_path}/{id}/metadata.json'))\n",
    "    control['id'] = id\n",
    "    localization['id'] = id\n",
    "    metadata['id'] = id\n",
    "    ctl.append(control)\n",
    "    lcz.append(localization)\n",
    "    mtd.append(metadata)\n",
    "    if id % 1000 == 0:\n",
    "        print(id)\n",
    "\n",
    "pd.concat(ctl).to_parquet(f'{test_data_path}/test_control.parquet', index=False)\n",
    "pd.concat(lcz).to_parquet(f'{test_data_path}/test_localization.parquet', index=False)\n",
    "pd.concat(mtd).to_parquet(f'{test_data_path}/test_metadata.parquet', index=False)\n",
    "# ch_client.insert_df(f'{db_name}.{table_name}', pd.concat(ctl))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = 'ycup'\n",
    "tables = ['test_control', 'test_localization', 'test_metadata']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2494 entries, 0 to 2493\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   stamp_ns  2494 non-null   int64  \n",
      " 1   x         2494 non-null   float64\n",
      " 2   y         2494 non-null   float64\n",
      " 3   z         2494 non-null   float64\n",
      " 4   roll      2494 non-null   float64\n",
      " 5   pitch     2494 non-null   float64\n",
      " 6   yaw       2494 non-null   float64\n",
      " 7   id        2494 non-null   int64  \n",
      "dtypes: float64(6), int64(2)\n",
      "memory usage: 156.0 KB\n"
     ]
    }
   ],
   "source": [
    "pd.read_parquet(f'{test_data_path}/test_localization.parquet').info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'ycup'\n",
    "table_name = 'test_control'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
