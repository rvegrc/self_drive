{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DATA_FOLDER = \"./YandexCup2024\"\n",
    "\n",
    "TRAIN_DATASET_PATH = os.path.join(ROOT_DATA_FOLDER, \"YaCupTrain\")\n",
    "TEST_DATASET_PATH = os.path.join(ROOT_DATA_FOLDER, \"YaCupTest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all ids of a dataset\n",
    "\n",
    "def read_testcase_ids(dataset_path: str):\n",
    "    ids = [int(case_id) for case_id in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, case_id))]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = read_testcase_ids(TRAIN_DATASET_PATH)\n",
    "len(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = read_testcase_ids(TEST_DATASET_PATH)\n",
    "len(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFilePaths:\n",
    "    def __init__(self, testcase_path: str):\n",
    "        self.testcase_path = testcase_path\n",
    "        \n",
    "    def localization(self):\n",
    "        return os.path.join(self.testcase_path, 'localization.csv')\n",
    "    \n",
    "    def control(self):\n",
    "        return os.path.join(self.testcase_path, 'control.csv')\n",
    "    \n",
    "    def metadata(self):\n",
    "        return os.path.join(self.testcase_path, 'metadata.json')\n",
    "    \n",
    "    # exists only for test_dataset\n",
    "    def requested_stamps(self):\n",
    "        return os.path.join(self.testcase_path, 'requested_stamps.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def read_localization(localization_path: str):\n",
    "    return pd.read_csv(localization_path)\n",
    "\n",
    "def read_control(control_path):\n",
    "    return pd.read_csv(control_path)\n",
    "\n",
    "def read_metadata(metadata_path: str):\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def read_requested_stamps(requested_stamps_path: str):\n",
    "    return pd.read_csv(requested_stamps_path)\n",
    "    \n",
    "def read_testcase(dataset_path: str, testcase_id: str, is_test: bool = False):\n",
    "    testcase_path = os.path.join(dataset_path, str(testcase_id))\n",
    "    data_file_paths = DataFilePaths(testcase_path)\n",
    "    \n",
    "    testcase_data = {}\n",
    "    testcase_data['localization'] = read_localization(data_file_paths.localization())\n",
    "    testcase_data['control'] = read_control(data_file_paths.control())\n",
    "    testcase_data['metadata'] = read_metadata(data_file_paths.metadata())\n",
    "    if is_test:\n",
    "        testcase_data['requested_stamps'] = read_requested_stamps(data_file_paths.requested_stamps())\n",
    "        \n",
    "    return testcase_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_testcases(dataset_path: str, is_test: bool = False, testcase_ids: typing.Iterable[int] = None):\n",
    "    result = {}\n",
    "    if testcase_ids is None:\n",
    "        testcase_ids = read_testcase_ids(dataset_path)\n",
    "    \n",
    "    for testcase_id in tqdm(testcase_ids):\n",
    "        testcase = read_testcase(dataset_path, testcase_id, is_test=is_test)\n",
    "        result[testcase_id] = testcase\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may take some time\n",
    "\n",
    "train_dataset = read_testcases(TRAIN_DATASET_PATH)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = read_testcases(TEST_DATASET_PATH, is_test=True)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy baseline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NSECS_IN_SEC = 1000000000\n",
    "\n",
    "def secs_to_nsecs(secs: float):\n",
    "    return int(secs * NSECS_IN_SEC)\n",
    "\n",
    "def nsecs_to_secs(nsecs: int):\n",
    "    return float(nsecs) / NSECS_IN_SEC\n",
    "\n",
    "def yaw_direction(yaw_value):\n",
    "    return np.array([np.cos(yaw_value), np.sin(yaw_value)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple pose prediction logic without taking into account control states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_df_to_poses(loc_df):\n",
    "    poses = []\n",
    "    for stamp_ns, x, y, yaw in zip(loc_df['stamp_ns'], loc_df['x'], loc_df['y'], loc_df['yaw']):\n",
    "        poses.append({'stamp_ns': stamp_ns, 'pos': np.array([x, y]), 'yaw': yaw})\n",
    "    return poses\n",
    "\n",
    "# naive estimation of speed at last known localization pose\n",
    "def dummy_estimate_last_speed(localization_poses):\n",
    "    last_pose = localization_poses[-1]\n",
    "    \n",
    "    start_pose_idx = -1\n",
    "    for i, pose in enumerate(localization_poses, start=1-len(localization_poses)):\n",
    "        start_pose_idx = i\n",
    "        if nsecs_to_secs(last_pose['stamp_ns']) - nsecs_to_secs(pose['stamp_ns']) > 1.: # sec\n",
    "            break\n",
    "            \n",
    "    start_pose = localization_poses[start_pose_idx]\n",
    "    dt_sec = nsecs_to_secs(last_pose['stamp_ns']) - nsecs_to_secs(start_pose['stamp_ns'])\n",
    "    \n",
    "    if dt_sec > 1e-5:\n",
    "        return np.linalg.norm(last_pose['pos'][:2] - start_pose['pos'][:2]) / dt_sec\n",
    "    return 5. # some default value\n",
    "\n",
    "def dummpy_predict_pose(last_loc_pose: dict, last_speed: float, prediction_stamp: int):\n",
    "    dt_sec = nsecs_to_secs(prediction_stamp) - nsecs_to_secs(last_loc_pose['stamp_ns'])\n",
    "    distance = dt_sec * last_speed\n",
    "    direction = yaw_direction(last_loc_pose['yaw'])\n",
    "    pos_translate = direction * distance\n",
    "    return {\"pos\": last_loc_pose['pos'] + pos_translate, 'yaw': last_loc_pose['yaw']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_testcase(testcase: dict):\n",
    "    loc_df = testcase['localization']\n",
    "    localization_poses = localization_df_to_poses(loc_df)\n",
    "    \n",
    "    last_loc_pose = localization_poses[-1]\n",
    "    last_speed = dummy_estimate_last_speed(localization_poses)\n",
    "    \n",
    "    predicted_poses = []\n",
    "    for stamp in testcase['requested_stamps']['stamp_ns']:\n",
    "        pose = dummpy_predict_pose(last_loc_pose, last_speed, stamp)\n",
    "        predicted_poses.append(pose)\n",
    "        \n",
    "    predictions = {}\n",
    "    predictions['stamp_ns'] = testcase['requested_stamps']['stamp_ns']\n",
    "    predictions['x'] = [pose['pos'][0] for pose in predicted_poses]\n",
    "    predictions['y'] = [pose['pos'][1] for pose in predicted_poses]\n",
    "    predictions['yaw'] = [pose['yaw'] for pose in predicted_poses]\n",
    "    return pd.DataFrame(predictions)\n",
    "\n",
    "def predict_test_dataset(test_dataset: dict):\n",
    "    predictions = {}\n",
    "    for testcase_id, testcase in tqdm(test_dataset.items()): \n",
    "        predictions[testcase_id] = predict_testcase(testcase)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make prediction for requested stamps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_predictions = predict_test_dataset(test_dataset)\n",
    "len(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(dataset_predictions: dict, prediction_file_path: str):\n",
    "    prediction_list = []\n",
    "    for testcase_id, prediction in tqdm(dataset_predictions.items()):\n",
    "        prediction['testcase_id'] = [testcase_id] * len(prediction)\n",
    "        prediction_list.append(prediction)\n",
    "    predictions_df = pd.concat(prediction_list)\n",
    "    predictions_df = predictions_df.reindex(columns=[\"testcase_id\", \"stamp_ns\", \"x\", \"y\", \"yaw\"])\n",
    "    print(len(predictions_df))\n",
    "    predictions_df.to_csv(prediction_file_path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_predictions(test_predictions, os.path.join(ROOT_DATA_FOLDER, \"predictions.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's describe final metric. As a first step, all predicted triples $(x,y,yaw)$ are being converted into 2 points $[(x_1, y_1), (x_2, y_2)]$ in the following way:\n",
    "$$\n",
    "(x_1, y_1) = (x, y), \\\\\n",
    "(x_2, y_2) = (x_1, y_1) + S \\times (yaw_x, yaw_y)\n",
    "$$  \n",
    "\n",
    "where $S = 1$. In other words, we build a directed segment of length $1$. These points then used in the metric calculation.\n",
    "\n",
    "\n",
    "Metric for a single pose (rmse):\n",
    "\n",
    "$$\n",
    "pose\\_metric = \\sqrt{ \\frac{\\displaystyle\\sum_{j=1}^{k} {(x_j-\\hat{x_j})^2 + (y_j-\\hat{y_j})^2}}{k} }\n",
    "$$\n",
    "\n",
    "where $k$ - number of points that describe single pose (in our case $k=2$).\n",
    "\n",
    "Metric for a testcase:\n",
    "\n",
    "$$\n",
    "testcase\\_metric = \\frac{1}{n}  \\displaystyle\\sum_{i=1}^{n}pose\\_metric_i\n",
    "$$\n",
    "\n",
    "where $n$ - number of localization points to predict.\n",
    "\n",
    "And, final metric for a whole dataset:\n",
    "\n",
    "$$\n",
    "dataset\\_metric = \\frac{1}{n}  \\displaystyle\\sum_{i=1}^{n}testcase\\_metric_i\n",
    "$$\n",
    "\n",
    "where $n$ - number of test cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementation of the metric calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEGMENT_LENGTH = 1.\n",
    "\n",
    "def yaw_direction(yaw_value):\n",
    "    return np.array([np.cos(yaw_value), np.sin(yaw_value)])\n",
    "\n",
    "def build_car_points(x_y_yaw):\n",
    "    directions = np.vstack(yaw_direction(x_y_yaw[:, -1]))\n",
    "    \n",
    "    front_points = x_y_yaw[:, :-1] + SEGMENT_LENGTH * directions.T\n",
    "    points = np.vstack([x_y_yaw[:, :-1], front_points])\n",
    "    return points\n",
    "\n",
    "def build_car_points_from_merged_df(df: pd.DataFrame):\n",
    "    points_gt = df[['x_gt', 'y_gt', 'yaw_gt']].to_numpy()\n",
    "    points_pred = df[['x_pred', 'y_pred', 'yaw_pred']].to_numpy()\n",
    "    \n",
    "    points_gt = build_car_points(points_gt)\n",
    "    points_pred = build_car_points(points_pred)\n",
    "    return points_gt, points_pred\n",
    "\n",
    "def calculate_metric_testcase(df: pd.DataFrame):        \n",
    "    points_gt, points_pred = build_car_points_from_merged_df(df)\n",
    "    \n",
    "    metric = np.mean(np.sqrt(2. * np.mean((points_gt - points_pred) ** 2, axis=1)))\n",
    "    return metric\n",
    "\n",
    "def calculate_metric_dataset(ground_truth_df: pd.DataFrame, prediction_df: pd.DataFrame):\n",
    "    assert (len(ground_truth_df) == len(prediction_df))\n",
    "    \n",
    "    df = ground_truth_df.merge(prediction_df, on=['testcase_id', 'stamp_ns'], suffixes=['_gt', '_pred'])\n",
    "    \n",
    "    metric = df.groupby('testcase_id').apply(calculate_metric_testcase)\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
