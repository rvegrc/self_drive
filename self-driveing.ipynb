{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Предсказание движения беспилотного автомобиля\n",
    "\n",
    "Когда в XIX веке на улицах Великобритании появились первые самоходные повозки, они вызвали у людей скорее страх и недоверие, чем восторг. Поэтому в 1865 году в Великобритании был принят The Locomotive Act, более известный как Red Flag Act, который требовал, чтобы перед каждым автомобилем шёл человек с красным флажком или фонарём. Этот «предвестник прогресса» должен был предупреждать пешеходов и конные экипажи о приближении нового механического транспорта.\n",
    "\n",
    "Кроме того, закон строго ограничивал скорость автомобилей: не более 2 миль в час в городах и 4 миль в час за их пределами. Эти меры были направлены на то, чтобы адаптировать общество к новым транспортным средствам и минимизировать их риски для безопасности. К концу XIX века стало очевидно, что подобные ограничения только сдерживают прогресс, и в 1896 году Red Flag Act был отменён, а автомобили получили право двигаться быстрее и без «предвестника», предсказывающего появление автомобиля.\n",
    "\n",
    "Сегодня предсказание маршрута автомобиля стало делом не человека с флажком, а искусственного интеллекта. ИИ способен опираться на огромное количество данных — от состояния дорог и трафика до погодных условий и угла поворота колёс — чтобы не просто направить автомобиль, а выбрать для него наилучший маршрут.\n",
    "\n",
    "Ваша задача — обучить модель, позволяющую точно моделировать траекторию движения автомобиля на основе поступающих команд управления, технических характеристик и исторических данных о прошлых проездах транспорта по различным дорогам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные для обучения\n",
    "Архив YaCupTrain.tar содержит набор из N train записанных сцен проезда легкового автомобиля, разложенных по отдельным папкам. Каждая папка содержит 3 файла:\n",
    "\n",
    "- metadata.json: содержит общую информацию про сцену\n",
    "- ride_date — дата проезда\n",
    "- vehicle_id — уникальный идентификатор автомобиля\n",
    "- vehicle_model — идентификатор модели автомобиля\n",
    "- vehicle_model_modification — идентификатор модификации указанной модели автомобиля\n",
    "- tires — идентификатор типа шин, используемых для колёс передней (front) и задней (rear) оси автомобиля\n",
    "- location_reference_point_id — идентификатор референсной точки, используемой в качестве начала отсчёта координат в файле localization.csv\n",
    "- localization.csv: описывает траекторию движения автомобиля на данной 60-секундной сцене. Представляет собой csv файл, каждая строчка которого имеет формат\n",
    "stamp_ns — время в наносекундах от начала сцены\n",
    "x, y, z — координаты центра задней оси автомобиля. Считаются в метрах от указанной референсной точки сцены. Направления осей относительно референсной точки: \n",
    "x - на восток, \n",
    "y - на север, \n",
    "z - в небо\n",
    "roll, pitch, yaw — углы Эйлера в радианах, описывающие ориентацию автомобиля в пространстве. Угол yaw считается относительно оси \n",
    "x в направлении оси y.\n",
    "- control.csv: описывает последовательность команд управления, отправленных автомобилю на протяжении данной сцены.\n",
    "- stamp_ns — время в наносекундах от начала сцены\n",
    "- acceleration_level — желаемая интенсивность ускорения. Положительные значения соответствуют силе нажатия на педаль газа, отрицательные — силе нажатия на педаль тормоза\n",
    "- steering — желаемый угол поворота руля в градусах относительно центрального положения\n",
    "Обратите внимание, что диапазон значений acceleration_level зависит от модели автомобиля. Также, важно отметить, что данные команды описывают желаемое целевое состояние элементов управления в указанный момент времени, и не обязательно исполняются мгновенно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные для тестирования\n",
    "Архив YaCupTest.tar содержит набор из N test    сцен, для которых требуется предсказать новую траекторию автомобиля на основе начального состояния и поступающих команд управления. Каждая папка с тестовым сценарием содержит 4 файла:\n",
    "\n",
    "- metadata.json: содержит общую информацию про сцену аналогично обучающим данным\n",
    "- localization.csv: описывает траекторию движения автомобиля в течении первых 5 секунд сцены. Формат аналогичен обучающим данным.\n",
    "- control.csv: описывает последовательность команд управления в течении первых 20 секунд сцены. Формат аналогичен обучающим данным.\n",
    "- requested_stamps.csv: содержит одну колонку stamp_ns, содержащую список из T n  моментов времени от начала сцены (в наносекундах) в интервале с 5 по 20 секунду, для которых требуется предсказать положение автомобиля.\n",
    "\n",
    "## Формат вывода\n",
    "В качестве решения вам необходимо отправить один файл в формате *.csv, содержащий следующие 5 колонок:\n",
    "\n",
    "- testcase_id — номер сцены из тестового набора (имя папки от 0 до N test −1)\n",
    "- stamp_ns — моменты времени из соответствующего файла requested_stamps.csv тестовой сцены.\n",
    "- x, y, yaw — 3 колонки с предсказанными координатами положения машины и её ориентации на плоскости в указанные моменты времени (В формате аналогичном входным данным).\n",
    "Таким образом, общее количество строк с предсказаниями в файле с ответом должно совпадать с суммарным количеством таймстемпов в файлах requested_stamps.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x, y, yaw target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's describe final metric. As a first step, all predicted triples $(x,y,yaw)$ are being converted into 2 points $[(x_1, y_1), (x_2, y_2)]$ in the following way:\n",
    "$$\n",
    "(x_1, y_1) = (x, y), \\\\\n",
    "(x_2, y_2) = (x_1, y_1) + S \\times (yaw_x, yaw_y)\n",
    "$$  \n",
    "\n",
    "where $S = 1$. In other words, we build a directed segment of length $1$. These points then used in the metric calculation.\n",
    "\n",
    "\n",
    "Metric for a single pose (rmse):\n",
    "\n",
    "$$\n",
    "pose\\_metric = \\sqrt{ \\frac{\\displaystyle\\sum_{j=1}^{k} {(x_j-\\hat{x_j})^2 + (y_j-\\hat{y_j})^2}}{k} }\n",
    "$$\n",
    "\n",
    "where $k$ - number of points that describe single pose (in our case $k=2$).\n",
    "\n",
    "Metric for a testcase:\n",
    "\n",
    "$$\n",
    "testcase\\_metric = \\frac{1}{n}  \\displaystyle\\sum_{i=1}^{n}pose\\_metric_i\n",
    "$$\n",
    "\n",
    "where $n$ - number of localization points to predict.\n",
    "\n",
    "And, final metric for a whole dataset:\n",
    "\n",
    "$$\n",
    "dataset\\_metric = \\frac{1}{n}  \\displaystyle\\sum_{i=1}^{n}testcase\\_metric_i\n",
    "$$\n",
    "\n",
    "where $n$ - number of test cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import clickhouse_connect\n",
    "import os\n",
    "import missingno as msno\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# constants\n",
    "CH_USER = os.getenv(\"CH_USER\")\n",
    "CH_PASS = os.getenv(\"CH_PASS\")\n",
    "CH_IP = os.getenv('CH_IP')\n",
    "\n",
    "# from tools import create_db_table_from_df, pd_tools, spark_tools\n",
    "\n",
    "root_path = \".\"\n",
    "tmp_path = f'{root_path}/tmp'\n",
    "data_path = f'{root_path}/data/self-drive'\n",
    "data_train_path = f'{data_path}/train_data'\n",
    "data_test_path = f'{data_path}/test_data'\n",
    "tmp_data_path=f'{data_path}/tmp_data'\n",
    "\n",
    "ch_client = clickhouse_connect.get_client(host=CH_IP, port=8123, username=CH_USER, password=CH_PASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initilize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.github.housepower#clickhouse-spark-runtime-3.4_2.12 added as a dependency\n",
      "com.clickhouse#clickhouse-jdbc added as a dependency\n",
      "com.clickhouse#clickhouse-http-client added as a dependency\n",
      "org.apache.httpcomponents.client5#httpclient5 added as a dependency\n",
      "com.github.housepower#clickhouse-native-jdbc added as a dependency\n",
      "ai.catboost#catboost-spark_3.4_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e2aa4386-3ac0-4580-a2b4-1b8534e5ea61;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.github.housepower#clickhouse-spark-runtime-3.4_2.12;0.7.3 in central\n",
      "\tfound com.clickhouse#clickhouse-jdbc;0.6.0-patch5 in central\n",
      "\tfound com.clickhouse#clickhouse-http-client;0.6.0-patch5 in central\n",
      "\tfound com.clickhouse#clickhouse-client;0.6.0-patch5 in central\n",
      "\tfound com.clickhouse#clickhouse-data;0.6.0-patch5 in central\n",
      "\tfound org.apache.httpcomponents.client5#httpclient5;5.3.1 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5;5.2.4 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5-h2;5.2.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound com.github.housepower#clickhouse-native-jdbc;2.7.1 in central\n",
      "\tfound io.airlift#aircompressor;0.21 in central\n",
      "\tfound ai.catboost#catboost-spark_3.4_2.13;1.2.7 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.13;2.6.0 in central\n",
      "\tfound com.google.guava#guava;32.0.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.33.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;2.8 in central\n",
      "\tfound commons-io#commons-io;2.7 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.11 in central\n",
      "\tfound org.apache.commons#commons-text;1.10.0 in central\n",
      "\tfound org.json4s#json4s-jackson_2.13;3.7.0-M11 in central\n",
      "\tfound org.json4s#json4s-core_2.13;3.7.0-M11 in central\n",
      "\tfound org.json4s#json4s-ast_2.13;3.7.0-M11 in central\n",
      "\tfound org.json4s#json4s-scalap_2.13;3.7.0-M11 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.14.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.14.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.2 in central\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-scala_2.13;2.14.2 in central\n",
      "\tfound io.github.classgraph#classgraph;4.8.98 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound ai.catboost#catboost-common;1.2.7 in central\n",
      "\tfound javax.validation#validation-api;1.1.0.Final in central\n",
      "\tfound ai.catboost#catboost-spark-macros_2.13;1.2.7 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.13.5 in central\n",
      ":: resolution report :: resolve 2411ms :: artifacts dl 103ms\n",
      "\t:: modules in use:\n",
      "\tai.catboost#catboost-common;1.2.7 from central in [default]\n",
      "\tai.catboost#catboost-spark-macros_2.13;1.2.7 from central in [default]\n",
      "\tai.catboost#catboost-spark_3.4_2.13;1.2.7 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-client;0.6.0-patch5 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-data;0.6.0-patch5 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-http-client;0.6.0-patch5 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-jdbc;0.6.0-patch5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.14.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.14.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-scala_2.13;2.14.2 from central in [default]\n",
      "\tcom.github.housepower#clickhouse-native-jdbc;2.7.1 from central in [default]\n",
      "\tcom.github.housepower#clickhouse-spark-runtime-3.4_2.12;0.7.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;32.0.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;2.8 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcommons-io#commons-io;2.7 from central in [default]\n",
      "\tio.airlift#aircompressor;0.21 from central in [default]\n",
      "\tio.github.classgraph#classgraph;4.8.98 from central in [default]\n",
      "\tjavax.validation#validation-api;1.1.0.Final from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.11 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.10.0 from central in [default]\n",
      "\torg.apache.httpcomponents.client5#httpclient5;5.3.1 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5;5.2.4 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5-h2;5.2.4 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.33.0 from central in [default]\n",
      "\torg.json4s#json4s-ast_2.13;3.7.0-M11 from central in [default]\n",
      "\torg.json4s#json4s-core_2.13;3.7.0-M11 from central in [default]\n",
      "\torg.json4s#json4s-jackson_2.13;3.7.0-M11 from central in [default]\n",
      "\torg.json4s#json4s-scalap_2.13;3.7.0-M11 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.13.5 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.13;2.6.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.commons#commons-lang3;3.12.0 by [org.apache.commons#commons-lang3;3.11] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.2 by [com.fasterxml.jackson.core#jackson-databind;2.14.2] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.25 by [org.slf4j#slf4j-api;1.7.36] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   41  |   0   |   0   |   3   ||   38  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e2aa4386-3ac0-4580-a2b4-1b8534e5ea61\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 38 already retrieved (0kB/45ms)\n",
      "24/10/30 18:36:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StructType, StructField, StringType, ArrayType, FloatType\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "\n",
    "\n",
    "import os\n",
    "#https://repo1.maven.org/maven2/com/github/housepower/clickhouse-native-jdbc/2.7.1/clickhouse-native-jdbc-2.7.1.jar\n",
    "packages = [\n",
    "    \"com.github.housepower:clickhouse-spark-runtime-3.4_2.12:0.7.3\"\n",
    "    ,\"com.clickhouse:clickhouse-jdbc:0.6.0-patch5\"\n",
    "    ,\"com.clickhouse:clickhouse-http-client:0.6.0-patch5\"\n",
    "    ,\"org.apache.httpcomponents.client5:httpclient5:5.3.1\"\n",
    "    ,\"com.github.housepower:clickhouse-native-jdbc:2.7.1\"\n",
    "    ,\"ai.catboost:catboost-spark_3.4_2.13:1.2.7\"\n",
    "\n",
    "]\n",
    "ram = 30\n",
    "cpu = 22*3\n",
    "# Define the application name and setup session\n",
    "appName = \"Connect To ClickHouse via PySpark\"\n",
    "spark = (SparkSession.builder\n",
    "         .appName(appName)\n",
    "         .config(\"spark.jars.packages\", \",\".join(packages))\n",
    "         .config(\"spark.sql.catalog.clickhouse\", \"xenon.clickhouse.ClickHouseCatalog\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.host\", CH_IP)\n",
    "         .config(\"spark.sql.catalog.clickhouse.protocol\", \"http\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.http_port\", \"8123\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.user\", CH_USER)\n",
    "         .config(\"spark.sql.catalog.clickhouse.password\", CH_PASS)\n",
    "         .config(\"spark.sql.catalog.clickhouse.database\", \"default\")\n",
    "        #  .config(\"spark.spark.clickhouse.write.compression.codec\", \"lz4\")\n",
    "        #  .config(\"spark.clickhouse.read.compression.codec\", \"lz4\")\n",
    "        #  .config(\"spark.clickhouse.write.format\", \"arrow\")\n",
    "         #    .config(\"spark.clickhouse.write.distributed.convertLocal\", \"true\") l\n",
    "         #    .config(\"spark.clickhouse.write.repartitionNum\", \"1\") \n",
    "         #.config(\"spark.clickhouse.write.maxRetry\", \"1000\")\n",
    "         #    .config(\"spark.clickhouse.write.repartitionStrictly\", \"true\") \n",
    "         #    .config(\"spark.clickhouse.write.distributed.useClusterNodes\", \"false\") \n",
    "        #  .config(\"spark.clickhouse.write.batchSize\", \"1000000\")\n",
    "         #.config(\"spark.sql.catalog.clickhouse.socket_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.sql.catalog.clickhouse.connection_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.sql.catalog.clickhouse.query_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.clickhouse.options.socket_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.clickhouse.options.connection_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.clickhouse.options.query_timeout\", \"600000000\")         \n",
    "         .config(\"spark.executor.memory\", f\"{ram}g\")\n",
    "        #  .config(\"spark.executor.cores\", \"5\")\n",
    "         .config(\"spark.driver.maxResultSize\", f\"{ram}g\")\n",
    "        #  .config(\"spark.driver.memory\", f\"{ram}g\")\n",
    "        #  .config(\"spark.executor.memoryOverhead\", f\"{ram}g\")\n",
    "        #  .config(\"spark.sql.debug.maxToStringFields\", \"100000\")\n",
    "         .getOrCreate()\n",
    "         )\n",
    "#SedonaRegistrator.registerAll(spark)\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse\", \"xenon.clickhouse.ClickHouseCatalog\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.host\", \"127.0.0.1\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.protocol\", \"http\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.http_port\", \"8123\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.user\", \"default\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.password\", \"\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.database\", \"default\")\n",
    "import catboost_spark\n",
    "\n",
    "spark.sql(\"use clickhouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            0\n",
       "1            1\n",
       "2            2\n",
       "3            3\n",
       "4            4\n",
       "         ...  \n",
       "41995    41995\n",
       "41996    41996\n",
       "41997    41997\n",
       "41998    41998\n",
       "41999    41999\n",
       "Length: 42000, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read folder names in path\n",
    "def read_names(path: str):\n",
    "    '''Read folder names or file names in the path'''\n",
    "    return os.listdir(path)\n",
    "\n",
    "train_ids = pd.Series(read_names(data_train_path)).apply(int)\n",
    "test_ids = pd.Series(read_names(data_test_path)).apply(int)\n",
    "train_ids = train_ids.sort_values().reset_index(drop=True)\n",
    "train_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['control.csv', 'localization.csv', 'metadata.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_temp = read_names(f'{data_train_path}/{train_ids[0]}')\n",
    "files_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stamp_ns</th>\n",
       "      <th>acceleration_level</th>\n",
       "      <th>steering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987440736</td>\n",
       "      <td>-114</td>\n",
       "      <td>-2.655140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3027341070</td>\n",
       "      <td>-123</td>\n",
       "      <td>-2.598169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3066793076</td>\n",
       "      <td>-132</td>\n",
       "      <td>-2.544422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3106757146</td>\n",
       "      <td>-141</td>\n",
       "      <td>-2.544422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3146784622</td>\n",
       "      <td>-147</td>\n",
       "      <td>-2.488557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     stamp_ns  acceleration_level  steering\n",
       "0  2987440736                -114 -2.655140\n",
       "1  3027341070                -123 -2.598169\n",
       "2  3066793076                -132 -2.544422\n",
       "3  3106757146                -141 -2.544422\n",
       "4  3146784622                -147 -2.488557"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_temp = pd.read_csv(f'{data_train_path}/{train_ids[0]}/{files_temp[0]}')\n",
    "control_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stamp_ns</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>roll</th>\n",
       "      <th>pitch</th>\n",
       "      <th>yaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-4292.313705</td>\n",
       "      <td>-14527.266319</td>\n",
       "      <td>66.043314</td>\n",
       "      <td>0.003926</td>\n",
       "      <td>-0.054198</td>\n",
       "      <td>-1.936810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39989868</td>\n",
       "      <td>-4292.489928</td>\n",
       "      <td>-14527.726083</td>\n",
       "      <td>66.070022</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>-0.054172</td>\n",
       "      <td>-1.936858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79819886</td>\n",
       "      <td>-4292.662729</td>\n",
       "      <td>-14528.183063</td>\n",
       "      <td>66.090338</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>-0.054628</td>\n",
       "      <td>-1.936827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125154671</td>\n",
       "      <td>-4292.862032</td>\n",
       "      <td>-14528.702952</td>\n",
       "      <td>66.120814</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>-0.054559</td>\n",
       "      <td>-1.936894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>159636974</td>\n",
       "      <td>-4293.011898</td>\n",
       "      <td>-14529.097871</td>\n",
       "      <td>66.138226</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>-0.053668</td>\n",
       "      <td>-1.936876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stamp_ns            x             y          z      roll     pitch   \n",
       "0          0 -4292.313705 -14527.266319  66.043314  0.003926 -0.054198  \\\n",
       "1   39989868 -4292.489928 -14527.726083  66.070022  0.003702 -0.054172   \n",
       "2   79819886 -4292.662729 -14528.183063  66.090338  0.002404 -0.054628   \n",
       "3  125154671 -4292.862032 -14528.702952  66.120814  0.002709 -0.054559   \n",
       "4  159636974 -4293.011898 -14529.097871  66.138226  0.003264 -0.053668   \n",
       "\n",
       "        yaw  \n",
       "0 -1.936810  \n",
       "1 -1.936858  \n",
       "2 -1.936827  \n",
       "3 -1.936894  \n",
       "4 -1.936876  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localization_temp = pd.read_csv(f'{data_train_path}/{train_ids[0]}/{files_temp[1]}')\n",
    "localization_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_date</th>\n",
       "      <th>tires</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>vehicle_model</th>\n",
       "      <th>vehicle_model_modification</th>\n",
       "      <th>location_reference_point_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>front</th>\n",
       "      <td>2022-03-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rear</th>\n",
       "      <td>2022-03-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ride_date  tires  vehicle_id  vehicle_model   \n",
       "front  2022-03-14      0           0              0  \\\n",
       "rear   2022-03-14      0           0              0   \n",
       "\n",
       "       vehicle_model_modification  location_reference_point_id  \n",
       "front                           0                            0  \n",
       "rear                            0                            0  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_temp = pd.read_json(f'{data_train_path}/{train_ids[0]}/{files_temp[2]}')\n",
    "metadata_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pd_df_model(control:pd.DataFrame, localization:pd.DataFrame, metadata:pd.DataFrame):\n",
    "    '''Make a model pandas dataframe from control, localization and metadata dataframes'''\n",
    "\n",
    "    def find_min_max(control:pd.DataFrame, localization:pd.DataFrame):\n",
    "        '''Find min and max timestamp in localization for each timestamp in control dataframe'''\n",
    "        control['loc_stamp_max'] = control['stamp_ns'].apply(lambda x: localization[localization['stamp_ns'] >= x]['stamp_ns'].min())\n",
    "        control['loc_stamp_min'] = control['stamp_ns'].apply(lambda x: localization[localization['stamp_ns'] < x]['stamp_ns'].max())\n",
    "        control_2m = control.copy()\n",
    "        return control_2m\n",
    "\n",
    "    def merge_min_max(control_2m:pd.DataFrame, localization:pd.DataFrame):\n",
    "        '''Merge min and max timestamp in localization for each timestamp in control dataframe'''\n",
    "        control_3m = (control_2m.merge(localization, left_on='loc_stamp_max', right_on='stamp_ns', how='left', suffixes=('', '_max'))\n",
    "                    .merge(localization, left_on='loc_stamp_min', right_on='stamp_ns', how='left', suffixes=('', '_min'))\n",
    "        )\n",
    "        control_3m.rename(columns={'x':'x_max', 'y':'y_max', 'z':'z_max', 'roll':'roll_max', 'pitch':'pitch_max', 'yaw':'yaw_max'}, inplace=True)\n",
    "        control_3m.drop(columns=['loc_stamp_max', 'loc_stamp_min'], inplace=True)\n",
    "        return control_3m\n",
    "\n",
    "    def interpolate_coords(control_3m, col_min:str, col_max:str):\n",
    "        '''Interpolate values between max and min values'''\n",
    "        control_inter = (control_3m[['stamp_ns', 'stamp_ns_max', 'stamp_ns_min', col_min, col_max]]\n",
    "                    .apply(lambda x: (x['stamp_ns'] - x['stamp_ns_min']) / (x['stamp_ns_max'] - x['stamp_ns_min']) * (x[col_max] - x[col_min]) + x[col_min], axis=1)\n",
    "                    )\n",
    "        return control_inter\n",
    "\n",
    "    control_2m = find_min_max(control, localization)\n",
    "    control_3m = merge_min_max(control_2m, localization)\n",
    "\n",
    "    coords_cols = ['x', 'y', 'z', 'roll', 'pitch', 'yaw']\n",
    "    contr_cols = ['stamp_ns', 'acceleration_level', 'steering']\n",
    "\n",
    "    for col in coords_cols:\n",
    "        control_3m[col] = interpolate_coords(control_3m, f'{col}_min', f'{col}_max')\n",
    "\n",
    "    control_inter = control_3m[contr_cols + coords_cols]\n",
    "\n",
    "\n",
    "    def tires_to_columns_date(metadata:pd.DataFrame):\n",
    "        '''Change tires column to front and rear columns and \n",
    "        convert ride_date to datetime and add year, month, day columns'''\n",
    "        metadata['front_tire'] = metadata['tires'][0]\n",
    "        metadata['rear_tire'] = metadata['tires'][1]\n",
    "        metadata = metadata.drop(columns=['tires']).reset_index(drop=True).loc[:0]\n",
    "        # convert ride_date to datetime and add year, month, day columns\n",
    "        metadata['ride_date'] = pd.to_datetime(metadata['ride_date'])\n",
    "        metadata['ride_year'] = metadata['ride_date'].dt.year\n",
    "        metadata['ride_month'] = metadata['ride_date'].dt.month\n",
    "        metadata['ride_day'] = metadata['ride_date'].dt.day\n",
    "        metadata = metadata.drop(columns=['ride_date'])\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "    # add metada to each row in control dataframe\n",
    "    def add_metadata(control:pd.DataFrame, metadata:pd.DataFrame):\n",
    "        '''Add metada to each row in control dataframe'''\n",
    "        # Make a copy to avoid SettingWithCopyWarning\n",
    "        control_model = control.copy()\n",
    "        for col in metadata.columns:\n",
    "            control_model[col] = metadata.loc[0, col]  # Set the entire column in the copy\n",
    "        \n",
    "        return control_model\n",
    "\n",
    "\n",
    "    \n",
    "    metadata_m = tires_to_columns_date(metadata)\n",
    "\n",
    "    control_model = add_metadata(control_inter, metadata_m)\n",
    "\n",
    "    return control_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stamp_ns</th>\n",
       "      <th>acceleration_level</th>\n",
       "      <th>steering</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>roll</th>\n",
       "      <th>pitch</th>\n",
       "      <th>yaw</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>vehicle_model</th>\n",
       "      <th>vehicle_model_modification</th>\n",
       "      <th>location_reference_point_id</th>\n",
       "      <th>front_tire</th>\n",
       "      <th>rear_tire</th>\n",
       "      <th>ride_year</th>\n",
       "      <th>ride_month</th>\n",
       "      <th>ride_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987440736</td>\n",
       "      <td>-114</td>\n",
       "      <td>-2.655140</td>\n",
       "      <td>-4305.325027</td>\n",
       "      <td>-14560.800637</td>\n",
       "      <td>67.786293</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>-0.048921</td>\n",
       "      <td>-1.945764</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3027341070</td>\n",
       "      <td>-123</td>\n",
       "      <td>-2.598169</td>\n",
       "      <td>-4305.489155</td>\n",
       "      <td>-14561.217631</td>\n",
       "      <td>67.808224</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>-0.049162</td>\n",
       "      <td>-1.945839</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3066793076</td>\n",
       "      <td>-132</td>\n",
       "      <td>-2.544422</td>\n",
       "      <td>-4305.652097</td>\n",
       "      <td>-14561.630123</td>\n",
       "      <td>67.833736</td>\n",
       "      <td>0.005068</td>\n",
       "      <td>-0.049696</td>\n",
       "      <td>-1.945933</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3106757146</td>\n",
       "      <td>-141</td>\n",
       "      <td>-2.544422</td>\n",
       "      <td>-4305.815555</td>\n",
       "      <td>-14562.044470</td>\n",
       "      <td>67.857731</td>\n",
       "      <td>0.006305</td>\n",
       "      <td>-0.050110</td>\n",
       "      <td>-1.946037</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3146784622</td>\n",
       "      <td>-147</td>\n",
       "      <td>-2.488557</td>\n",
       "      <td>-4305.979063</td>\n",
       "      <td>-14562.457108</td>\n",
       "      <td>67.880212</td>\n",
       "      <td>0.007713</td>\n",
       "      <td>-0.049996</td>\n",
       "      <td>-1.946176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     stamp_ns  acceleration_level  steering            x             y   \n",
       "0  2987440736                -114 -2.655140 -4305.325027 -14560.800637  \\\n",
       "1  3027341070                -123 -2.598169 -4305.489155 -14561.217631   \n",
       "2  3066793076                -132 -2.544422 -4305.652097 -14561.630123   \n",
       "3  3106757146                -141 -2.544422 -4305.815555 -14562.044470   \n",
       "4  3146784622                -147 -2.488557 -4305.979063 -14562.457108   \n",
       "\n",
       "           z      roll     pitch       yaw  vehicle_id  vehicle_model   \n",
       "0  67.786293  0.002836 -0.048921 -1.945764           0              0  \\\n",
       "1  67.808224  0.002993 -0.049162 -1.945839           0              0   \n",
       "2  67.833736  0.005068 -0.049696 -1.945933           0              0   \n",
       "3  67.857731  0.006305 -0.050110 -1.946037           0              0   \n",
       "4  67.880212  0.007713 -0.049996 -1.946176           0              0   \n",
       "\n",
       "   vehicle_model_modification  location_reference_point_id  front_tire   \n",
       "0                           0                            0           0  \\\n",
       "1                           0                            0           0   \n",
       "2                           0                            0           0   \n",
       "3                           0                            0           0   \n",
       "4                           0                            0           0   \n",
       "\n",
       "   rear_tire  ride_year  ride_month  ride_day  \n",
       "0          0       2022           3        14  \n",
       "1          0       2022           3        14  \n",
       "2          0       2022           3        14  \n",
       "3          0       2022           3        14  \n",
       "4          0       2022           3        14  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "control_model = make_df_model(control_temp, localization_temp, metadata_temp)\n",
    "control_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_read_metadata(data_path:str, train_ids:pd.Series, n_ids:int, files:pd.Series):\n",
    "    '''Read metadata file in spark'''\n",
    "    metadata_schema = StructType([\n",
    "        StructField(\"ride_date\", StringType(), True),\n",
    "        StructField(\"tires\", StructType([\n",
    "            StructField(\"front\", IntegerType(), True),\n",
    "            StructField(\"rear\", IntegerType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"vehicle_id\", IntegerType(), True),\n",
    "        StructField(\"vehicle_model\", IntegerType(), True),\n",
    "        StructField(\"vehicle_model_modification\", IntegerType(), True),\n",
    "        StructField(\"location_reference_point_id\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    metadata = (spark.read.json(f'{data_path}/{train_ids[n_ids]}/{files[2]}', schema=metadata_schema, multiLine=True)\n",
    "                .withColumn('front_tire', F.col('tires.front'))\n",
    "                .withColumn('rear_tire', F.col('tires.rear'))\n",
    "                .withColumn('ride_year', F.year('ride_date'))\n",
    "                .withColumn('ride_month', F.month('ride_date'))\n",
    "                .withColumn('ride_day', F.dayofmonth('ride_date'))\n",
    "                .drop('tires', 'ride_date')\n",
    "    )\n",
    "    \n",
    "     \n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------------+---------------------------+----------+---------+---------+----------+--------+\n",
      "|vehicle_id|vehicle_model|vehicle_model_modification|location_reference_point_id|front_tire|rear_tire|ride_year|ride_month|ride_day|\n",
      "+----------+-------------+--------------------------+---------------------------+----------+---------+---------+----------+--------+\n",
      "|        45|            0|                         0|                          0|         0|        0|     2021|         8|       9|\n",
      "+----------+-------------+--------------------------+---------------------------+----------+---------+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_read_metadata(data_train_path, train_ids, 78, files_temp).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_sp = spark.read.csv(f'{data_train_path}/{train_ids[0]}/{files_temp[0]}', header=True, inferSchema=True)\n",
    "localization_sp = spark.read.csv(f'{data_train_path}/{train_ids[0]}/{files_temp[1]}', header=True, inferSchema=True)\n",
    "metadata_sp = spark_read_metadata(data_train_path, train_ids, 0, files_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_max(control, localization):\n",
    "    '''Find min and max timestamp in localization for each timestamp in control dataframe'''\n",
    "    \n",
    "    # Join control with localization to find the closest timestamps\n",
    "    control_with_min_max = control.alias('control').join(\n",
    "        localization.alias('localization'),\n",
    "        on=F.col('localization.stamp_ns') >= F.col('control.stamp_ns'),\n",
    "        how='left'\n",
    "    ).withColumn(\n",
    "        'loc_stamp_max',\n",
    "        F.min('localization.stamp_ns').over(Window.partitionBy('control.stamp_ns'))\n",
    "    ).join(\n",
    "        localization.alias('localization_min'),\n",
    "        on=F.col('localization_min.stamp_ns') < F.col('control.stamp_ns'),\n",
    "        how='left'\n",
    "    ).withColumn(\n",
    "        'loc_stamp_min',\n",
    "        F.max('localization_min.stamp_ns').over(Window.partitionBy('control.stamp_ns'))\n",
    "    )\n",
    "\n",
    "    return control_with_min_max.select('control.*', 'loc_stamp_min', 'loc_stamp_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spark_df_model(control:SparkDataFrame, localization:SparkDataFrame, metadata:SparkDataFrame):\n",
    "\n",
    "    \n",
    "    def find_min_max(control, localization):\n",
    "        '''Find min and max timestamp in localization for each timestamp in control dataframe'''\n",
    "        \n",
    "        # Join control with localization to find the closest timestamps\n",
    "        control_with_min_max = control.alias('control').join(\n",
    "            localization.alias('localization'),\n",
    "            on=F.col('localization.stamp_ns') >= F.col('control.stamp_ns'),\n",
    "            how='left'\n",
    "        ).withColumn(\n",
    "            'loc_stamp_max',\n",
    "            F.min('localization.stamp_ns').over(Window.partitionBy('control.stamp_ns'))\n",
    "        ).join(\n",
    "            localization.alias('localization_min'),\n",
    "            on=F.col('localization_min.stamp_ns') < F.col('control.stamp_ns'),\n",
    "            how='left'\n",
    "        ).withColumn(\n",
    "            'loc_stamp_min',\n",
    "            F.max('localization_min.stamp_ns').over(Window.partitionBy('control.stamp_ns'))\n",
    "        )\n",
    "\n",
    "        return control_with_min_max.select('control.*', 'loc_stamp_min', 'loc_stamp_max')\n",
    "    \n",
    "    def merge_min_max(control_2m, localization):\n",
    "        '''Merge min and max timestamp in localization for each timestamp in control dataframe'''\n",
    "        \n",
    "        control_3m = control_2m.join(\n",
    "            localization.withColumnRenamed('stamp_ns', 'stamp_ns_max').alias('localization_max'),\n",
    "            on=control_2m['loc_stamp_max'] == F.col('localization_max.stamp_ns_max'),\n",
    "            how='left'\n",
    "        ).join(\n",
    "            localization.withColumnRenamed('stamp_ns', 'stamp_ns_min').alias('localization_min'),\n",
    "            on=control_2m['loc_stamp_min'] == F.col('localization_min.stamp_ns_min'),\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Rename columns for clarity\n",
    "        for col in ['x', 'y', 'z', 'roll', 'pitch', 'yaw']:\n",
    "            control_3m = control_3m.withColumnRenamed(f'localization_max.{col}', f'{col}_max')\n",
    "            control_3m = control_3m.withColumnRenamed(f'localization_min.{col}', f'{col}_min')\n",
    "\n",
    "        return control_3m.drop('loc_stamp_max', 'loc_stamp_min')\n",
    "    \n",
    "    def interpolate_coords(control_3m, col_min, col_max):\n",
    "        '''Interpolate values between max and min values'''\n",
    "        \n",
    "        interpolation_expr = (\n",
    "            (F.col('stamp_ns') - F.col('stamp_ns_min')) / (F.col('stamp_ns_max') - F.col('stamp_ns_min')) *\n",
    "            (F.col(col_max) - F.col(col_min)) + F.col(col_min)\n",
    "        )\n",
    "        \n",
    "        return control_3m.withColumn(col_min.split('_')[0], interpolation_expr)\n",
    "\n",
    "    control_2m = find_min_max(control, localization)\n",
    "    control_3m = merge_min_max(control_2m, localization)\n",
    "\n",
    "    # Interpolate each coordinate column\n",
    "    coords_cols = ['x', 'y', 'z', 'roll', 'pitch', 'yaw']\n",
    "    for col in coords_cols:\n",
    "        control_3m = interpolate_coords(control_3m, f'{col}_min', f'{col}_max')\n",
    "    \n",
    "    contr_cols = ['stamp_ns', 'acceleration_level', 'steering']\n",
    "    control_inter = control_3m.select(*contr_cols, *coords_cols)\n",
    "    \n",
    "    def tires_to_columns_date(metadata):\n",
    "        '''Change tires column to front and rear columns and convert ride_date to datetime and add year, month, day columns'''\n",
    "        \n",
    "        # Add columns for front and rear tires\n",
    "        metadata_with_tires = metadata.withColumn('front_tire', F.col('tires')[0]) \\\n",
    "                                      .withColumn('rear_tire', F.col('tires')[1]) \\\n",
    "                                      .drop('tires')\n",
    "        \n",
    "        # Convert ride_date to datetime and extract year, month, day\n",
    "        metadata_with_tires = metadata_with_tires.withColumn('ride_date', F.to_date(F.col('ride_date'))) \\\n",
    "                                                 .withColumn('ride_year', F.year(F.col('ride_date'))) \\\n",
    "                                                 .withColumn('ride_month', F.month(F.col('ride_date'))) \\\n",
    "                                                 .withColumn('ride_day', F.dayofmonth(F.col('ride_date'))) \\\n",
    "                                                 .drop('ride_date')\n",
    "        \n",
    "        return metadata_with_tires\n",
    "\n",
    "    def add_metadata(control, metadata):\n",
    "        '''Add metadata to each row in control dataframe'''\n",
    "        \n",
    "        metadata_row = metadata.first()  # Assuming only one row for metadata\n",
    "        for col in metadata.columns:\n",
    "            control = control.withColumn(col, F.lit(metadata_row[col]))\n",
    "        \n",
    "        return control\n",
    "\n",
    "    # Process metadata\n",
    "    metadata_m = tires_to_columns_date(metadata)\n",
    "    \n",
    "    # Add metadata to each row in the control DataFrame\n",
    "    control_model = add_metadata(control_inter, metadata_m)\n",
    "\n",
    "    return control_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `x_max` cannot be resolved. Did you mean one of the following? [`localization_max`.`x`, `localization_max`.`y`, `localization_max`.`z`, `control`.`stamp_ns`, `localization_min`.`x`].;\n'Project [stamp_ns#846L, acceleration_level#847, steering#848, stamp_ns_max#1062L, (((cast((stamp_ns#846L - stamp_ns_min#1102L) as double) / cast((stamp_ns_max#1062L - stamp_ns_min#1102L) as double)) * ('x_max - 'x_min)) + 'x_min) AS x#1401, y#1072, z#1073, roll#1074, pitch#1075, yaw#1076, stamp_ns_min#1102L, (((cast((stamp_ns#846L - stamp_ns_min#1102L) as double) / cast((stamp_ns_max#1062L - stamp_ns_min#1102L) as double)) * ('x_max - 'x_min)) + 'x_min) AS x#1402, y#1112, z#1113, roll#1114, pitch#1115, yaw#1116]\n+- Project [stamp_ns#846L, acceleration_level#847, steering#848, stamp_ns_max#1062L, x#1071, y#1072, z#1073, roll#1074, pitch#1075, yaw#1076, stamp_ns_min#1102L, x#1111, y#1112, z#1113, roll#1114, pitch#1115, yaw#1116]\n   +- Join LeftOuter, (loc_stamp_min#1037L = stamp_ns_min#1102L)\n      :- Join LeftOuter, (loc_stamp_max#980L = stamp_ns_max#1062L)\n      :  :- Project [stamp_ns#846L, acceleration_level#847, steering#848, loc_stamp_min#1037L, loc_stamp_max#980L]\n      :  :  +- Project [stamp_ns#846L, acceleration_level#847, steering#848, stamp_ns#869L, x#870, y#871, z#872, roll#873, pitch#874, yaw#875, loc_stamp_max#980L, stamp_ns#992L, x#993, y#994, z#995, roll#996, pitch#997, yaw#998, loc_stamp_min#1037L]\n      :  :     +- Project [stamp_ns#846L, acceleration_level#847, steering#848, stamp_ns#869L, x#870, y#871, z#872, roll#873, pitch#874, yaw#875, loc_stamp_max#980L, stamp_ns#992L, x#993, y#994, z#995, roll#996, pitch#997, yaw#998, loc_stamp_min#1037L, loc_stamp_min#1037L]\n      :  :        +- Window [max(stamp_ns#992L) windowspecdefinition(stamp_ns#846L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS loc_stamp_min#1037L], [stamp_ns#846L]\n      :  :           +- Project [stamp_ns#846L, acceleration_level#847, steering#848, stamp_ns#869L, x#870, y#871, z#872, roll#873, pitch#874, yaw#875, loc_stamp_max#980L, stamp_ns#992L, x#993, y#994, z#995, roll#996, pitch#997, yaw#998]\n      :  :              +- Join LeftOuter, (stamp_ns#992L < stamp_ns#846L)\n      :  :                 :- Project [stamp_ns#846L, acceleration_level#847, steering#848, stamp_ns#869L, x#870, y#871, z#872, roll#873, pitch#874, yaw#875, loc_stamp_max#980L]\n      :  :                 :  +- Project [stamp_ns#846L, acceleration_level#847, steering#848, stamp_ns#869L, x#870, y#871, z#872, roll#873, pitch#874, yaw#875, loc_stamp_max#980L, loc_stamp_max#980L]\n      :  :                 :     +- Window [min(stamp_ns#869L) windowspecdefinition(stamp_ns#846L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS loc_stamp_max#980L], [stamp_ns#846L]\n      :  :                 :        +- Project [stamp_ns#846L, acceleration_level#847, steering#848, stamp_ns#869L, x#870, y#871, z#872, roll#873, pitch#874, yaw#875]\n      :  :                 :           +- Join LeftOuter, (stamp_ns#869L >= stamp_ns#846L)\n      :  :                 :              :- SubqueryAlias control\n      :  :                 :              :  +- Relation [stamp_ns#846L,acceleration_level#847,steering#848] csv\n      :  :                 :              +- SubqueryAlias localization\n      :  :                 :                 +- Relation [stamp_ns#869L,x#870,y#871,z#872,roll#873,pitch#874,yaw#875] csv\n      :  :                 +- SubqueryAlias localization_min\n      :  :                    +- Relation [stamp_ns#992L,x#993,y#994,z#995,roll#996,pitch#997,yaw#998] csv\n      :  +- SubqueryAlias localization_max\n      :     +- Project [stamp_ns#1070L AS stamp_ns_max#1062L, x#1071, y#1072, z#1073, roll#1074, pitch#1075, yaw#1076]\n      :        +- Relation [stamp_ns#1070L,x#1071,y#1072,z#1073,roll#1074,pitch#1075,yaw#1076] csv\n      +- SubqueryAlias localization_min\n         +- Project [stamp_ns#1110L AS stamp_ns_min#1102L, x#1111, y#1112, z#1113, roll#1114, pitch#1115, yaw#1116]\n            +- Relation [stamp_ns#1110L,x#1111,y#1112,z#1113,roll#1114,pitch#1115,yaw#1116] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m localization_sp \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_train_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_ids[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfiles_temp[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m metadata_sp \u001b[38;5;241m=\u001b[39m spark_read_metadata(data_train_path, train_ids, \u001b[38;5;241m0\u001b[39m, files_temp)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmake_spark_df_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontrol_sp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocalization_sp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_sp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[16], line 62\u001b[0m, in \u001b[0;36mmake_spark_df_model\u001b[0;34m(control, localization, metadata)\u001b[0m\n\u001b[1;32m     60\u001b[0m coords_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroll\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpitch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myaw\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m coords_cols:\n\u001b[0;32m---> 62\u001b[0m     control_3m \u001b[38;5;241m=\u001b[39m \u001b[43minterpolate_coords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontrol_3m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcol\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_min\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcol\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_max\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m contr_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstamp_ns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration_level\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     65\u001b[0m control_inter \u001b[38;5;241m=\u001b[39m control_3m\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39mcontr_cols, \u001b[38;5;241m*\u001b[39mcoords_cols)\n",
      "Cell \u001b[0;32mIn[16], line 54\u001b[0m, in \u001b[0;36mmake_spark_df_model.<locals>.interpolate_coords\u001b[0;34m(control_3m, col_min, col_max)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Interpolate values between max and min values'''\u001b[39;00m\n\u001b[1;32m     49\u001b[0m interpolation_expr \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     50\u001b[0m     (F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstamp_ns\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstamp_ns_min\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;241m/\u001b[39m (F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstamp_ns_max\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstamp_ns_min\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     51\u001b[0m     (F\u001b[38;5;241m.\u001b[39mcol(col_max) \u001b[38;5;241m-\u001b[39m F\u001b[38;5;241m.\u001b[39mcol(col_min)) \u001b[38;5;241m+\u001b[39m F\u001b[38;5;241m.\u001b[39mcol(col_min)\n\u001b[1;32m     52\u001b[0m )\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontrol_3m\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_min\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation_expr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:4789\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   4784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   4785\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   4786\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4787\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   4788\u001b[0m     )\n\u001b[0;32m-> 4789\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `x_max` cannot be resolved. Did you mean one of the following? [`localization_max`.`x`, `localization_max`.`y`, `localization_max`.`z`, `control`.`stamp_ns`, `localization_min`.`x`].;\n'Project [stamp_ns#846L, acceleration_level#847, steering#848, stamp_ns_max#1062L, (((cast((stamp_ns#846L - stamp_ns_min#1102L) as double) / cast((stamp_ns_max#1062L - stamp_ns_min#1102L) as double)) * ('x_max - 'x_min)) + 'x_min) AS x#1401, y#1072, z#1073, roll#1074, pitch#1075, yaw#1076, stamp_ns_min#1102L, (((cast((stamp_ns#846L - stamp_ns_min#1102L) as double) / cast((stamp_ns_max#1062L - stamp_ns_min#1102L) as double)) * ('x_max - 'x_min)) + 'x_min) AS x#1402, y#1112, z#1113, roll#1114, pitch#1115, yaw#1116]\n+- Project [stamp_ns#846L, acceleration_level#847, steering#848, stamp_ns_max#1062L, x#1071, y#1072, z#1073, roll#1074, pitch#1075, yaw#1076, stamp_ns_min#1102L, x#1111, y#1112, z#1113, roll#1114, pitch#1115, yaw#1116]\n   +- Join LeftOuter, (loc_stamp_min#1037L = stamp_ns_min#1102L)\n      :- Join LeftOuter, (loc_stamp_max#980L = stamp_ns_max#1062L)\n      :  :- Project [stamp_ns#846L, acceleration_level#847, steering#848, loc_stamp_min#1037L, loc_stamp_max#980L]\n      :  :  +- Project [stamp_ns#846L, acceleration_level#847, steering#848, stamp_ns#869L, x#870, y#871, z#872, roll#873, pitch#874, yaw#875, loc_stamp_max#980L, stamp_ns#992L, x#993, y#994, z#995, roll#996, pitch#997, yaw#998, loc_stamp_min#1037L]\n      :  :     +- Project [stamp_ns#846L, acceleration_level#847, steering#848, stamp_ns#869L, x#870, y#871, z#872, roll#873, pitch#874, yaw#875, loc_stamp_max#980L, stamp_ns#992L, x#993, y#994, z#995, roll#996, pitch#997, yaw#998, loc_stamp_min#1037L, loc_stamp_min#1037L]\n      :  :        +- Window [max(stamp_ns#992L) windowspecdefinition(stamp_ns#846L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS loc_stamp_min#1037L], [stamp_ns#846L]\n      :  :           +- Project [stamp_ns#846L, acceleration_level#847, steering#848, stamp_ns#869L, x#870, y#871, z#872, roll#873, pitch#874, yaw#875, loc_stamp_max#980L, stamp_ns#992L, x#993, y#994, z#995, roll#996, pitch#997, yaw#998]\n      :  :              +- Join LeftOuter, (stamp_ns#992L < stamp_ns#846L)\n      :  :                 :- Project [stamp_ns#846L, acceleration_level#847, steering#848, stamp_ns#869L, x#870, y#871, z#872, roll#873, pitch#874, yaw#875, loc_stamp_max#980L]\n      :  :                 :  +- Project [stamp_ns#846L, acceleration_level#847, steering#848, stamp_ns#869L, x#870, y#871, z#872, roll#873, pitch#874, yaw#875, loc_stamp_max#980L, loc_stamp_max#980L]\n      :  :                 :     +- Window [min(stamp_ns#869L) windowspecdefinition(stamp_ns#846L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS loc_stamp_max#980L], [stamp_ns#846L]\n      :  :                 :        +- Project [stamp_ns#846L, acceleration_level#847, steering#848, stamp_ns#869L, x#870, y#871, z#872, roll#873, pitch#874, yaw#875]\n      :  :                 :           +- Join LeftOuter, (stamp_ns#869L >= stamp_ns#846L)\n      :  :                 :              :- SubqueryAlias control\n      :  :                 :              :  +- Relation [stamp_ns#846L,acceleration_level#847,steering#848] csv\n      :  :                 :              +- SubqueryAlias localization\n      :  :                 :                 +- Relation [stamp_ns#869L,x#870,y#871,z#872,roll#873,pitch#874,yaw#875] csv\n      :  :                 +- SubqueryAlias localization_min\n      :  :                    +- Relation [stamp_ns#992L,x#993,y#994,z#995,roll#996,pitch#997,yaw#998] csv\n      :  +- SubqueryAlias localization_max\n      :     +- Project [stamp_ns#1070L AS stamp_ns_max#1062L, x#1071, y#1072, z#1073, roll#1074, pitch#1075, yaw#1076]\n      :        +- Relation [stamp_ns#1070L,x#1071,y#1072,z#1073,roll#1074,pitch#1075,yaw#1076] csv\n      +- SubqueryAlias localization_min\n         +- Project [stamp_ns#1110L AS stamp_ns_min#1102L, x#1111, y#1112, z#1113, roll#1114, pitch#1115, yaw#1116]\n            +- Relation [stamp_ns#1110L,x#1111,y#1112,z#1113,roll#1114,pitch#1115,yaw#1116] csv\n"
     ]
    }
   ],
   "source": [
    "control_sp = spark.read.csv(f'{data_train_path}/{train_ids[0]}/{files_temp[0]}', header=True, inferSchema=True)\n",
    "localization_sp = spark.read.csv(f'{data_train_path}/{train_ids[0]}/{files_temp[1]}', header=True, inferSchema=True)\n",
    "metadata_sp = spark_read_metadata(data_train_path, train_ids, 0, files_temp)\n",
    "make_spark_df_model(control_sp, localization_sp, metadata_sp).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stamp_ns</th>\n",
       "      <th>acceleration_level</th>\n",
       "      <th>steering</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>roll</th>\n",
       "      <th>pitch</th>\n",
       "      <th>yaw</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>vehicle_model</th>\n",
       "      <th>vehicle_model_modification</th>\n",
       "      <th>location_reference_point_id</th>\n",
       "      <th>front_tire</th>\n",
       "      <th>rear_tire</th>\n",
       "      <th>ride_year</th>\n",
       "      <th>ride_month</th>\n",
       "      <th>ride_day</th>\n",
       "      <th>id</th>\n",
       "      <th>train_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987440736</td>\n",
       "      <td>-114</td>\n",
       "      <td>-2.655140</td>\n",
       "      <td>-4305.325027</td>\n",
       "      <td>-14560.800637</td>\n",
       "      <td>67.786293</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>-0.048921</td>\n",
       "      <td>-1.945764</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3027341070</td>\n",
       "      <td>-123</td>\n",
       "      <td>-2.598169</td>\n",
       "      <td>-4305.489155</td>\n",
       "      <td>-14561.217631</td>\n",
       "      <td>67.808224</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>-0.049162</td>\n",
       "      <td>-1.945839</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3066793076</td>\n",
       "      <td>-132</td>\n",
       "      <td>-2.544422</td>\n",
       "      <td>-4305.652097</td>\n",
       "      <td>-14561.630123</td>\n",
       "      <td>67.833736</td>\n",
       "      <td>0.005068</td>\n",
       "      <td>-0.049696</td>\n",
       "      <td>-1.945933</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3106757146</td>\n",
       "      <td>-141</td>\n",
       "      <td>-2.544422</td>\n",
       "      <td>-4305.815555</td>\n",
       "      <td>-14562.044470</td>\n",
       "      <td>67.857731</td>\n",
       "      <td>0.006305</td>\n",
       "      <td>-0.050110</td>\n",
       "      <td>-1.946037</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3146784622</td>\n",
       "      <td>-147</td>\n",
       "      <td>-2.488557</td>\n",
       "      <td>-4305.979063</td>\n",
       "      <td>-14562.457108</td>\n",
       "      <td>67.880212</td>\n",
       "      <td>0.007713</td>\n",
       "      <td>-0.049996</td>\n",
       "      <td>-1.946176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     stamp_ns  acceleration_level  steering            x             y   \n",
       "0  2987440736                -114 -2.655140 -4305.325027 -14560.800637  \\\n",
       "1  3027341070                -123 -2.598169 -4305.489155 -14561.217631   \n",
       "2  3066793076                -132 -2.544422 -4305.652097 -14561.630123   \n",
       "3  3106757146                -141 -2.544422 -4305.815555 -14562.044470   \n",
       "4  3146784622                -147 -2.488557 -4305.979063 -14562.457108   \n",
       "\n",
       "           z      roll     pitch       yaw  vehicle_id  vehicle_model   \n",
       "0  67.786293  0.002836 -0.048921 -1.945764           0              0  \\\n",
       "1  67.808224  0.002993 -0.049162 -1.945839           0              0   \n",
       "2  67.833736  0.005068 -0.049696 -1.945933           0              0   \n",
       "3  67.857731  0.006305 -0.050110 -1.946037           0              0   \n",
       "4  67.880212  0.007713 -0.049996 -1.946176           0              0   \n",
       "\n",
       "   vehicle_model_modification  location_reference_point_id  front_tire   \n",
       "0                           0                            0           0  \\\n",
       "1                           0                            0           0   \n",
       "2                           0                            0           0   \n",
       "3                           0                            0           0   \n",
       "4                           0                            0           0   \n",
       "\n",
       "   rear_tire  ride_year  ride_month  ride_day  id  train_id  \n",
       "0          0       2022           3        14   0         0  \n",
       "1          0       2022           3        14   0         0  \n",
       "2          0       2022           3        14   0         0  \n",
       "3          0       2022           3        14   0         0  \n",
       "4          0       2022           3        14   0         0  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_sp = spark.read.csv(f'{data_train_path}/{train_ids[0]}/{files_temp[0]}', header=True, inferSchema=True)\n",
    "localization_sp = spark.read.csv(f'{data_train_path}/{train_ids[0]}/{files_temp[1]}', header=True, inferSchema=True)\n",
    "metadata_sp = spark_read_metadata(data_train_path, train_ids, 0, files_temp)\n",
    "make_spark_df_model(control_sp, localization_sp, metadata_sp).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['control.csv', 'localization.csv', 'metadata.json']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data in each file\n",
    "def read_data_pandas(path: str, ids: pd.Series, files: list):\n",
    "    '''Read data in each file'''\n",
    "    for i in ids:\n",
    "        data = []\n",
    "        for file in files:\n",
    "            if file == 'control.csv':\n",
    "                data.append(pd.read_csv(f'{path}/{i}/{file}'))\n",
    "            elif file.endswith('.json'):\n",
    "                data.append(json.load(open(f'{path}/{i}/{file}')))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data in each file with spark\n",
    "def read_data_spark(path: str, ids: pd.Series, files: list):\n",
    "    '''Read data in each file with spark'''\n",
    "    for i in ids:\n",
    "        data = []\n",
    "        for file in files:\n",
    "            if file == 'control.csv':\n",
    "                conrtol = spark.read.csv(f'{path}/{i}/{file}', header=True, inferSchema=True)\n",
    "            elif file == 'localization.csv':\n",
    "                localization = spark.read.csv(f'{path}/{i}/{file}', header=True, inferSchema=True)\n",
    "            elif file == 'metadata.json':\n",
    "                metadata = spark.read.json(f'{path}/{i}/{file}', multiLine=True, mode='PERMISSIVE'))\n",
    "   \n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ids \u001b[38;5;241m=\u001b[39m train_ids[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[43mread_data_spark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles_temp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "ids = train_ids[:2]\n",
    "read_data_spark(data_train_path, ids, files_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(case_id) \u001b[38;5;28;01mfor\u001b[39;00m case_id \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(dataset_path) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, case_id))]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ids\n\u001b[0;32m----> 7\u001b[0m train_ids \u001b[38;5;241m=\u001b[39m \u001b[43mread_testcase_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m test_ids \u001b[38;5;241m=\u001b[39m read_testcase_ids(data_test_path)\n\u001b[1;32m     10\u001b[0m train_ids\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[116], line 4\u001b[0m, in \u001b[0;36mread_testcase_ids\u001b[0;34m(dataset_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_testcase_ids\u001b[39m(dataset_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(case_id) \u001b[38;5;28;01mfor\u001b[39;00m case_id \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(dataset_path) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, case_id))]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[0;32mIn[116], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_testcase_ids\u001b[39m(dataset_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(case_id) \u001b[38;5;28;01mfor\u001b[39;00m case_id \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(dataset_path) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcase_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "File \u001b[0;32m/usr/lib/python3.8/genericpath.py:42\u001b[0m, in \u001b[0;36misdir\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return true if the pathname refers to an existing directory.\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load all ids of a dataset\n",
    "\n",
    "def read_testcase_ids(dataset_path: str):\n",
    "    ids = [int(case_id) for case_id in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, case_id))]\n",
    "    return ids\n",
    "\n",
    "train_ids = read_testcase_ids(data_train_path)\n",
    "test_ids = read_testcase_ids(data_test_path)\n",
    "\n",
    "train_ids.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
