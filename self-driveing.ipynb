{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Предсказание движения беспилотного автомобиля\n",
    "\n",
    "Когда в XIX веке на улицах Великобритании появились первые самоходные повозки, они вызвали у людей скорее страх и недоверие, чем восторг. Поэтому в 1865 году в Великобритании был принят The Locomotive Act, более известный как Red Flag Act, который требовал, чтобы перед каждым автомобилем шёл человек с красным флажком или фонарём. Этот «предвестник прогресса» должен был предупреждать пешеходов и конные экипажи о приближении нового механического транспорта.\n",
    "\n",
    "Кроме того, закон строго ограничивал скорость автомобилей: не более 2 миль в час в городах и 4 миль в час за их пределами. Эти меры были направлены на то, чтобы адаптировать общество к новым транспортным средствам и минимизировать их риски для безопасности. К концу XIX века стало очевидно, что подобные ограничения только сдерживают прогресс, и в 1896 году Red Flag Act был отменён, а автомобили получили право двигаться быстрее и без «предвестника», предсказывающего появление автомобиля.\n",
    "\n",
    "Сегодня предсказание маршрута автомобиля стало делом не человека с флажком, а искусственного интеллекта. ИИ способен опираться на огромное количество данных — от состояния дорог и трафика до погодных условий и угла поворота колёс — чтобы не просто направить автомобиль, а выбрать для него наилучший маршрут.\n",
    "\n",
    "Ваша задача — обучить модель, позволяющую точно моделировать траекторию движения автомобиля на основе поступающих команд управления, технических характеристик и исторических данных о прошлых проездах транспорта по различным дорогам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные для обучения\n",
    "Архив YaCupTrain.tar содержит набор из N train записанных сцен проезда легкового автомобиля, разложенных по отдельным папкам. Каждая папка содержит 3 файла:\n",
    "\n",
    "- metadata.json: содержит общую информацию про сцену\n",
    "- ride_date — дата проезда\n",
    "- vehicle_id — уникальный идентификатор автомобиля\n",
    "- vehicle_model — идентификатор модели автомобиля\n",
    "- vehicle_model_modification — идентификатор модификации указанной модели автомобиля\n",
    "- tires — идентификатор типа шин, используемых для колёс передней (front) и задней (rear) оси автомобиля\n",
    "- location_reference_point_id — идентификатор референсной точки, используемой в качестве начала отсчёта координат в файле localization.csv\n",
    "- localization.csv: описывает траекторию движения автомобиля на данной 60-секундной сцене. Представляет собой csv файл, каждая строчка которого имеет формат\n",
    "stamp_ns — время в наносекундах от начала сцены\n",
    "x, y, z — координаты центра задней оси автомобиля. Считаются в метрах от указанной референсной точки сцены. Направления осей относительно референсной точки: \n",
    "x - на восток, \n",
    "y - на север, \n",
    "z - в небо\n",
    "roll, pitch, yaw — углы Эйлера в радианах, описывающие ориентацию автомобиля в пространстве. Угол yaw считается относительно оси \n",
    "x в направлении оси y.\n",
    "- control.csv: описывает последовательность команд управления, отправленных автомобилю на протяжении данной сцены.\n",
    "- stamp_ns — время в наносекундах от начала сцены\n",
    "- acceleration_level — желаемая интенсивность ускорения. Положительные значения соответствуют силе нажатия на педаль газа, отрицательные — силе нажатия на педаль тормоза\n",
    "- steering — желаемый угол поворота руля в градусах относительно центрального положения\n",
    "Обратите внимание, что диапазон значений acceleration_level зависит от модели автомобиля. Также, важно отметить, что данные команды описывают желаемое целевое состояние элементов управления в указанный момент времени, и не обязательно исполняются мгновенно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные для тестирования\n",
    "Архив YaCupTest.tar содержит набор из N test    сцен, для которых требуется предсказать новую траекторию автомобиля на основе начального состояния и поступающих команд управления. Каждая папка с тестовым сценарием содержит 4 файла:\n",
    "\n",
    "- metadata.json: содержит общую информацию про сцену аналогично обучающим данным\n",
    "- localization.csv: описывает траекторию движения автомобиля в течении первых 5 секунд сцены. Формат аналогичен обучающим данным.\n",
    "- control.csv: описывает последовательность команд управления в течении первых 20 секунд сцены. Формат аналогичен обучающим данным.\n",
    "- requested_stamps.csv: содержит одну колонку stamp_ns, содержащую список из T n  моментов времени от начала сцены (в наносекундах) в интервале с 5 по 20 секунду, для которых требуется предсказать положение автомобиля.\n",
    "\n",
    "## Формат вывода\n",
    "В качестве решения вам необходимо отправить один файл в формате *.csv, содержащий следующие 5 колонок:\n",
    "\n",
    "- testcase_id — номер сцены из тестового набора (имя папки от 0 до N test −1)\n",
    "- stamp_ns — моменты времени из соответствующего файла requested_stamps.csv тестовой сцены.\n",
    "- x, y, yaw — 3 колонки с предсказанными координатами положения машины и её ориентации на плоскости в указанные моменты времени (В формате аналогичном входным данным).\n",
    "Таким образом, общее количество строк с предсказаниями в файле с ответом должно совпадать с суммарным количеством таймстемпов в файлах requested_stamps.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x, y, yaw target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's describe final metric. As a first step, all predicted triples $(x,y,yaw)$ are being converted into 2 points $[(x_1, y_1), (x_2, y_2)]$ in the following way:\n",
    "$$\n",
    "(x_1, y_1) = (x, y), \\\\\n",
    "(x_2, y_2) = (x_1, y_1) + S \\times (yaw_x, yaw_y)\n",
    "$$  \n",
    "\n",
    "where $S = 1$. In other words, we build a directed segment of length $1$. These points then used in the metric calculation.\n",
    "\n",
    "\n",
    "Metric for a single pose (rmse):\n",
    "\n",
    "$$\n",
    "pose\\_metric = \\sqrt{ \\frac{\\displaystyle\\sum_{j=1}^{k} {(x_j-\\hat{x_j})^2 + (y_j-\\hat{y_j})^2}}{k} }\n",
    "$$\n",
    "\n",
    "where $k$ - number of points that describe single pose (in our case $k=2$).\n",
    "\n",
    "Metric for a testcase:\n",
    "\n",
    "$$\n",
    "testcase\\_metric = \\frac{1}{n}  \\displaystyle\\sum_{i=1}^{n}pose\\_metric_i\n",
    "$$\n",
    "\n",
    "where $n$ - number of localization points to predict.\n",
    "\n",
    "And, final metric for a whole dataset:\n",
    "\n",
    "$$\n",
    "dataset\\_metric = \\frac{1}{n}  \\displaystyle\\sum_{i=1}^{n}testcase\\_metric_i\n",
    "$$\n",
    "\n",
    "where $n$ - number of test cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pd_tools' from 'tools' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# load metrics\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, mean_absolute_error \u001b[38;5;28;01mas\u001b[39;00m mae, mean_absolute_percentage_error \u001b[38;5;28;01mas\u001b[39;00m mape\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pd_tools\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spark_tools\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# turn off warnings\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'pd_tools' from 'tools' (unknown location)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import clickhouse_connect\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# eda\n",
    "\n",
    "import phik\n",
    "\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# time\n",
    "from time import sleep\n",
    "from datetime import datetime as dt\n",
    "import pytz\n",
    "\n",
    "# ml\n",
    "import xgboost as xgb\n",
    "import catboost as ctb\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "import mlflow\n",
    "\n",
    "\n",
    "# mlflow.set_experiment('price_meas23_pred')\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline as skl_pipeline\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# from imblearn.pipeline import Pipeline as imb_pipeline\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer, OrdinalEncoder, StandardScaler, RobustScaler\n",
    "\n",
    "sklearn.set_config(transform_output='pandas')\n",
    "\n",
    "# load metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error as mae, mean_absolute_percentage_error as mape\n",
    "\n",
    "# turn off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# set all columns to be displayed\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# import tools\n",
    "\n",
    "from tools import pd_tools\n",
    "from tools import spark_tools\n",
    "from tools.create_db_table_from_df import sql_create_table_from_pd\n",
    "\n",
    "\n",
    "root_path = \".\"\n",
    "tmp_path = f'{root_path}/tmp'\n",
    "data_path = f'{root_path}/data/self-drive'\n",
    "data_train_path = f'{data_path}/train_data'\n",
    "data_test_path = f'{data_path}/test_data'\n",
    "tmp_data_path=f'{data_path}/tmp_data'\n",
    "\n",
    "# constants\n",
    "CH_USER = os.getenv(\"CH_USER\")\n",
    "CH_PASS = os.getenv(\"CH_PASS\")\n",
    "CH_IP = os.getenv('CH_IP')\n",
    "RAND_ST = 354\n",
    "# MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI')\n",
    "\n",
    "your_mlflow_tracking_uri = f'{root_path}/mlruns' # for docker mlflow server\n",
    "# your_mlflow_tracking_uri = \"http://127.0.0.1:5000\" # for local mlflow server\n",
    "# your_mlflow_tracking_uri = MLFLOW_TRACKING_URI # for remote mlflow server\n",
    "mlflow.set_tracking_uri(your_mlflow_tracking_uri)\n",
    "\n",
    "\n",
    "ch_client = clickhouse_connect.get_client(host=CH_IP, port=8123, username=CH_USER, password=CH_PASS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initilize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.github.housepower#clickhouse-spark-runtime-3.4_2.12 added as a dependency\n",
      "com.clickhouse#clickhouse-jdbc added as a dependency\n",
      "com.clickhouse#clickhouse-http-client added as a dependency\n",
      "org.apache.httpcomponents.client5#httpclient5 added as a dependency\n",
      "com.github.housepower#clickhouse-native-jdbc added as a dependency\n",
      "ai.catboost#catboost-spark_3.4_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e2aa4386-3ac0-4580-a2b4-1b8534e5ea61;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.github.housepower#clickhouse-spark-runtime-3.4_2.12;0.7.3 in central\n",
      "\tfound com.clickhouse#clickhouse-jdbc;0.6.0-patch5 in central\n",
      "\tfound com.clickhouse#clickhouse-http-client;0.6.0-patch5 in central\n",
      "\tfound com.clickhouse#clickhouse-client;0.6.0-patch5 in central\n",
      "\tfound com.clickhouse#clickhouse-data;0.6.0-patch5 in central\n",
      "\tfound org.apache.httpcomponents.client5#httpclient5;5.3.1 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5;5.2.4 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5-h2;5.2.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound com.github.housepower#clickhouse-native-jdbc;2.7.1 in central\n",
      "\tfound io.airlift#aircompressor;0.21 in central\n",
      "\tfound ai.catboost#catboost-spark_3.4_2.13;1.2.7 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.13;2.6.0 in central\n",
      "\tfound com.google.guava#guava;32.0.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.33.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;2.8 in central\n",
      "\tfound commons-io#commons-io;2.7 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.11 in central\n",
      "\tfound org.apache.commons#commons-text;1.10.0 in central\n",
      "\tfound org.json4s#json4s-jackson_2.13;3.7.0-M11 in central\n",
      "\tfound org.json4s#json4s-core_2.13;3.7.0-M11 in central\n",
      "\tfound org.json4s#json4s-ast_2.13;3.7.0-M11 in central\n",
      "\tfound org.json4s#json4s-scalap_2.13;3.7.0-M11 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.14.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.14.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.2 in central\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-scala_2.13;2.14.2 in central\n",
      "\tfound io.github.classgraph#classgraph;4.8.98 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound ai.catboost#catboost-common;1.2.7 in central\n",
      "\tfound javax.validation#validation-api;1.1.0.Final in central\n",
      "\tfound ai.catboost#catboost-spark-macros_2.13;1.2.7 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.13.5 in central\n",
      ":: resolution report :: resolve 2411ms :: artifacts dl 103ms\n",
      "\t:: modules in use:\n",
      "\tai.catboost#catboost-common;1.2.7 from central in [default]\n",
      "\tai.catboost#catboost-spark-macros_2.13;1.2.7 from central in [default]\n",
      "\tai.catboost#catboost-spark_3.4_2.13;1.2.7 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-client;0.6.0-patch5 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-data;0.6.0-patch5 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-http-client;0.6.0-patch5 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-jdbc;0.6.0-patch5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.14.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.14.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-scala_2.13;2.14.2 from central in [default]\n",
      "\tcom.github.housepower#clickhouse-native-jdbc;2.7.1 from central in [default]\n",
      "\tcom.github.housepower#clickhouse-spark-runtime-3.4_2.12;0.7.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;32.0.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;2.8 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcommons-io#commons-io;2.7 from central in [default]\n",
      "\tio.airlift#aircompressor;0.21 from central in [default]\n",
      "\tio.github.classgraph#classgraph;4.8.98 from central in [default]\n",
      "\tjavax.validation#validation-api;1.1.0.Final from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.11 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.10.0 from central in [default]\n",
      "\torg.apache.httpcomponents.client5#httpclient5;5.3.1 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5;5.2.4 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5-h2;5.2.4 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.33.0 from central in [default]\n",
      "\torg.json4s#json4s-ast_2.13;3.7.0-M11 from central in [default]\n",
      "\torg.json4s#json4s-core_2.13;3.7.0-M11 from central in [default]\n",
      "\torg.json4s#json4s-jackson_2.13;3.7.0-M11 from central in [default]\n",
      "\torg.json4s#json4s-scalap_2.13;3.7.0-M11 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.13.5 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.13;2.6.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.commons#commons-lang3;3.12.0 by [org.apache.commons#commons-lang3;3.11] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.2 by [com.fasterxml.jackson.core#jackson-databind;2.14.2] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.25 by [org.slf4j#slf4j-api;1.7.36] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   41  |   0   |   0   |   3   ||   38  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e2aa4386-3ac0-4580-a2b4-1b8534e5ea61\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 38 already retrieved (0kB/45ms)\n",
      "24/10/30 18:36:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StructType, StructField, StringType, ArrayType, FloatType\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "\n",
    "\n",
    "import os\n",
    "#https://repo1.maven.org/maven2/com/github/housepower/clickhouse-native-jdbc/2.7.1/clickhouse-native-jdbc-2.7.1.jar\n",
    "packages = [\n",
    "    \"com.github.housepower:clickhouse-spark-runtime-3.4_2.12:0.7.3\"\n",
    "    ,\"com.clickhouse:clickhouse-jdbc:0.6.0-patch5\"\n",
    "    ,\"com.clickhouse:clickhouse-http-client:0.6.0-patch5\"\n",
    "    ,\"org.apache.httpcomponents.client5:httpclient5:5.3.1\"\n",
    "    ,\"com.github.housepower:clickhouse-native-jdbc:2.7.1\"\n",
    "    ,\"ai.catboost:catboost-spark_3.4_2.13:1.2.7\"\n",
    "\n",
    "]\n",
    "ram = 30\n",
    "cpu = 22*3\n",
    "# Define the application name and setup session\n",
    "appName = \"Connect To ClickHouse via PySpark\"\n",
    "spark = (SparkSession.builder\n",
    "         .appName(appName)\n",
    "         .config(\"spark.jars.packages\", \",\".join(packages))\n",
    "         .config(\"spark.sql.catalog.clickhouse\", \"xenon.clickhouse.ClickHouseCatalog\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.host\", CH_IP)\n",
    "         .config(\"spark.sql.catalog.clickhouse.protocol\", \"http\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.http_port\", \"8123\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.user\", CH_USER)\n",
    "         .config(\"spark.sql.catalog.clickhouse.password\", CH_PASS)\n",
    "         .config(\"spark.sql.catalog.clickhouse.database\", \"default\")\n",
    "        #  .config(\"spark.spark.clickhouse.write.compression.codec\", \"lz4\")\n",
    "        #  .config(\"spark.clickhouse.read.compression.codec\", \"lz4\")\n",
    "        #  .config(\"spark.clickhouse.write.format\", \"arrow\")\n",
    "         #    .config(\"spark.clickhouse.write.distributed.convertLocal\", \"true\") l\n",
    "         #    .config(\"spark.clickhouse.write.repartitionNum\", \"1\") \n",
    "         #.config(\"spark.clickhouse.write.maxRetry\", \"1000\")\n",
    "         #    .config(\"spark.clickhouse.write.repartitionStrictly\", \"true\") \n",
    "         #    .config(\"spark.clickhouse.write.distributed.useClusterNodes\", \"false\") \n",
    "        #  .config(\"spark.clickhouse.write.batchSize\", \"1000000\")\n",
    "         #.config(\"spark.sql.catalog.clickhouse.socket_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.sql.catalog.clickhouse.connection_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.sql.catalog.clickhouse.query_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.clickhouse.options.socket_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.clickhouse.options.connection_timeout\", \"600000000\")\n",
    "        #  .config(\"spark.clickhouse.options.query_timeout\", \"600000000\")         \n",
    "         .config(\"spark.executor.memory\", f\"{ram}g\")\n",
    "        #  .config(\"spark.executor.cores\", \"5\")\n",
    "         .config(\"spark.driver.maxResultSize\", f\"{ram}g\")\n",
    "        #  .config(\"spark.driver.memory\", f\"{ram}g\")\n",
    "        #  .config(\"spark.executor.memoryOverhead\", f\"{ram}g\")\n",
    "        #  .config(\"spark.sql.debug.maxToStringFields\", \"100000\")\n",
    "         .getOrCreate()\n",
    "         )\n",
    "#SedonaRegistrator.registerAll(spark)\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse\", \"xenon.clickhouse.ClickHouseCatalog\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.host\", \"127.0.0.1\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.protocol\", \"http\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.http_port\", \"8123\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.user\", \"default\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.password\", \"\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.database\", \"default\")\n",
    "import catboost_spark\n",
    "\n",
    "spark.sql(\"use clickhouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            0\n",
       "1            1\n",
       "2            2\n",
       "3            3\n",
       "4            4\n",
       "         ...  \n",
       "41995    41995\n",
       "41996    41996\n",
       "41997    41997\n",
       "41998    41998\n",
       "41999    41999\n",
       "Length: 42000, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read folder names in path\n",
    "def read_names(path: str):\n",
    "    '''Read folder names or file names in the path'''\n",
    "    return os.listdir(path)\n",
    "\n",
    "train_ids = pd.Series(read_names(data_train_path)).apply(int)\n",
    "test_ids = pd.Series(read_names(data_test_path)).apply(int)\n",
    "train_ids = train_ids.sort_values().reset_index(drop=True)\n",
    "train_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['control.csv', 'localization.csv', 'metadata.json']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_temp = read_names(f'{data_train_path}/{train_ids[0]}')\n",
    "files_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stamp_ns</th>\n",
       "      <th>acceleration_level</th>\n",
       "      <th>steering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987440736</td>\n",
       "      <td>-114</td>\n",
       "      <td>-2.655140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3027341070</td>\n",
       "      <td>-123</td>\n",
       "      <td>-2.598169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3066793076</td>\n",
       "      <td>-132</td>\n",
       "      <td>-2.544422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3106757146</td>\n",
       "      <td>-141</td>\n",
       "      <td>-2.544422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3146784622</td>\n",
       "      <td>-147</td>\n",
       "      <td>-2.488557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     stamp_ns  acceleration_level  steering\n",
       "0  2987440736                -114 -2.655140\n",
       "1  3027341070                -123 -2.598169\n",
       "2  3066793076                -132 -2.544422\n",
       "3  3106757146                -141 -2.544422\n",
       "4  3146784622                -147 -2.488557"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_temp = pd.read_csv(f'{data_train_path}/{train_ids[0]}/{files_temp[0]}')\n",
    "control_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stamp_ns</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>roll</th>\n",
       "      <th>pitch</th>\n",
       "      <th>yaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-4292.313705</td>\n",
       "      <td>-14527.266319</td>\n",
       "      <td>66.043314</td>\n",
       "      <td>0.003926</td>\n",
       "      <td>-0.054198</td>\n",
       "      <td>-1.936810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39989868</td>\n",
       "      <td>-4292.489928</td>\n",
       "      <td>-14527.726083</td>\n",
       "      <td>66.070022</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>-0.054172</td>\n",
       "      <td>-1.936858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79819886</td>\n",
       "      <td>-4292.662729</td>\n",
       "      <td>-14528.183063</td>\n",
       "      <td>66.090338</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>-0.054628</td>\n",
       "      <td>-1.936827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125154671</td>\n",
       "      <td>-4292.862032</td>\n",
       "      <td>-14528.702952</td>\n",
       "      <td>66.120814</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>-0.054559</td>\n",
       "      <td>-1.936894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>159636974</td>\n",
       "      <td>-4293.011898</td>\n",
       "      <td>-14529.097871</td>\n",
       "      <td>66.138226</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>-0.053668</td>\n",
       "      <td>-1.936876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stamp_ns            x             y          z      roll     pitch   \n",
       "0          0 -4292.313705 -14527.266319  66.043314  0.003926 -0.054198  \\\n",
       "1   39989868 -4292.489928 -14527.726083  66.070022  0.003702 -0.054172   \n",
       "2   79819886 -4292.662729 -14528.183063  66.090338  0.002404 -0.054628   \n",
       "3  125154671 -4292.862032 -14528.702952  66.120814  0.002709 -0.054559   \n",
       "4  159636974 -4293.011898 -14529.097871  66.138226  0.003264 -0.053668   \n",
       "\n",
       "        yaw  \n",
       "0 -1.936810  \n",
       "1 -1.936858  \n",
       "2 -1.936827  \n",
       "3 -1.936894  \n",
       "4 -1.936876  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localization_temp = pd.read_csv(f'{data_train_path}/{train_ids[0]}/{files_temp[1]}')\n",
    "localization_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_date</th>\n",
       "      <th>tires</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>vehicle_model</th>\n",
       "      <th>vehicle_model_modification</th>\n",
       "      <th>location_reference_point_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>front</th>\n",
       "      <td>2022-03-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rear</th>\n",
       "      <td>2022-03-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ride_date  tires  vehicle_id  vehicle_model   \n",
       "front  2022-03-14      0           0              0  \\\n",
       "rear   2022-03-14      0           0              0   \n",
       "\n",
       "       vehicle_model_modification  location_reference_point_id  \n",
       "front                           0                            0  \n",
       "rear                            0                            0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_temp = pd.read_json(f'{data_train_path}/{train_ids[0]}/{files_temp[2]}')\n",
    "metadata_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sp_df_id(control:pd.DataFrame, localization:pd.DataFrame, metadata:pd.DataFrame) -> SparkDataFrame:\n",
    "    '''Make a model spark dataframe for from control, localization and metadata dataframes for a single id'''\n",
    "\n",
    "    def find_min_max(control:pd.DataFrame, localization:pd.DataFrame):\n",
    "        '''Find min and max timestamp in localization for each timestamp in control dataframe'''\n",
    "        control['loc_stamp_max'] = control['stamp_ns'].apply(lambda x: localization[localization['stamp_ns'] >= x]['stamp_ns'].min())\n",
    "        control['loc_stamp_min'] = control['stamp_ns'].apply(lambda x: localization[localization['stamp_ns'] < x]['stamp_ns'].max())\n",
    "        control_2m = control.copy()\n",
    "        return control_2m\n",
    "\n",
    "    def merge_min_max(control_2m:pd.DataFrame, localization:pd.DataFrame):\n",
    "        '''Merge min and max timestamp in localization for each timestamp in control dataframe'''\n",
    "        control_3m = (control_2m.merge(localization, left_on='loc_stamp_max', right_on='stamp_ns', how='left', suffixes=('', '_max'))\n",
    "                    .merge(localization, left_on='loc_stamp_min', right_on='stamp_ns', how='left', suffixes=('', '_min'))\n",
    "        )\n",
    "        control_3m.rename(columns={'x':'x_max', 'y':'y_max', 'z':'z_max', 'roll':'roll_max', 'pitch':'pitch_max', 'yaw':'yaw_max'}, inplace=True)\n",
    "        control_3m.drop(columns=['loc_stamp_max', 'loc_stamp_min'], inplace=True)\n",
    "        return control_3m\n",
    "\n",
    "    def interpolate_coords(control_3m, col_min:str, col_max:str):\n",
    "        '''Interpolate values between max and min values'''\n",
    "        control_inter = (control_3m[['stamp_ns', 'stamp_ns_max', 'stamp_ns_min', col_min, col_max]]\n",
    "                    .apply(lambda x: (x['stamp_ns'] - x['stamp_ns_min']) / (x['stamp_ns_max'] - x['stamp_ns_min']) * (x[col_max] - x[col_min]) + x[col_min], axis=1)\n",
    "                    )\n",
    "        return control_inter\n",
    "\n",
    "    control_2m = find_min_max(control, localization)\n",
    "    control_3m = merge_min_max(control_2m, localization)\n",
    "\n",
    "    coords_cols = ['x', 'y', 'z', 'roll', 'pitch', 'yaw']\n",
    "    contr_cols = ['stamp_ns', 'acceleration_level', 'steering']\n",
    "\n",
    "    for col in coords_cols:\n",
    "        control_3m[col] = interpolate_coords(control_3m, f'{col}_min', f'{col}_max')\n",
    "\n",
    "    control_inter = control_3m[contr_cols + coords_cols]\n",
    "\n",
    "\n",
    "    def tires_to_columns_date(metadata:pd.DataFrame):\n",
    "        '''Change tires column to front and rear columns and \n",
    "        convert ride_date to datetime and add year, month, day columns'''\n",
    "        metadata['front_tire'] = metadata['tires'][0]\n",
    "        metadata['rear_tire'] = metadata['tires'][1]\n",
    "        metadata = metadata.drop(columns=['tires']).reset_index(drop=True).loc[:0]\n",
    "        # convert ride_date to datetime and add year, month, day columns\n",
    "        metadata['ride_date'] = pd.to_datetime(metadata['ride_date'])\n",
    "        metadata['ride_year'] = metadata['ride_date'].dt.year\n",
    "        metadata['ride_month'] = metadata['ride_date'].dt.month\n",
    "        metadata['ride_day'] = metadata['ride_date'].dt.day\n",
    "        metadata = metadata.drop(columns=['ride_date'])\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "    # add metada to each row in control dataframe\n",
    "    def add_metadata(control:pd.DataFrame, metadata:pd.DataFrame):\n",
    "        '''Add metada to each row in control dataframe'''\n",
    "        # Make a copy to avoid SettingWithCopyWarning\n",
    "        control_model = control.copy()\n",
    "        for col in metadata.columns:\n",
    "            control_model[col] = metadata.loc[0, col]  # Set the entire column in the copy\n",
    "        \n",
    "        return control_model\n",
    "\n",
    "\n",
    "    \n",
    "    metadata_m = tires_to_columns_date(metadata)\n",
    "\n",
    "    # create spark dataframe for model\n",
    "  \n",
    "    control_model = spark.createDataFrame(add_metadata(control_inter, metadata_m))\n",
    "\n",
    "    return control_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_sp_read(data_path:str, ids:pd.Series, n_ids:int, meatadata_file:str) -> SparkDataFrame:\n",
    "    '''Read metadata json file in spark'''\n",
    "    metadata_schema = StructType([\n",
    "        StructField(\"ride_date\", StringType(), True),\n",
    "        StructField(\"tires\", StructType([\n",
    "            StructField(\"front\", IntegerType(), True),\n",
    "            StructField(\"rear\", IntegerType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"vehicle_id\", IntegerType(), True),\n",
    "        StructField(\"vehicle_model\", IntegerType(), True),\n",
    "        StructField(\"vehicle_model_modification\", IntegerType(), True),\n",
    "        StructField(\"location_reference_point_id\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    metadata = (spark.read.json(f'{data_path}/{ids[n_ids]}/{meatadata_file}', schema=metadata_schema, multiLine=True)\n",
    "                .withColumn('front_tire', F.col('tires.front'))\n",
    "                .withColumn('rear_tire', F.col('tires.rear'))\n",
    "                .withColumn('ride_year', F.year('ride_date'))\n",
    "                .withColumn('ride_month', F.month('ride_date'))\n",
    "                .withColumn('ride_day', F.dayofmonth('ride_date'))\n",
    "                .drop('tires', 'ride_date')\n",
    "    )\n",
    "    \n",
    "     \n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data in each file with spark\n",
    "def make_sp_df_all_ids(path: str, ids: pd.Series, files: list):\n",
    "    '''Read data in each file with spark'''\n",
    "    for i in ids:\n",
    "        data = []\n",
    "        for file in files:\n",
    "            if file == 'control.csv':\n",
    "                control = pd.read_csv(f'{path}/{i}/{file}')\n",
    "            elif file == 'localization.csv':\n",
    "                localization = pd.read_csv(f'{path}/{i}/{file}')\n",
    "            elif file == 'metadata.json':\n",
    "                metadata = pd.read_json(f'{path}/{i}/{file}')\n",
    "        data.append(make_sp_df_id(control, localization, metadata).withColumn('id', F.lit(i)))\n",
    "    \n",
    "    # Concatenate all DataFrames in the list\n",
    "    data_all = data[0]\n",
    "    for df in data[1:]:\n",
    "        data_all = data_all.unionByName(df)\n",
    "     \n",
    "    return data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+-------------------+-------------------+-----------------+--------------------+--------------------+------------------+----------+-------------+--------------------------+---------------------------+----------+---------+---------+----------+--------+---+\n",
      "|  stamp_ns|acceleration_level|          steering|                  x|                  y|                z|                roll|               pitch|               yaw|vehicle_id|vehicle_model|vehicle_model_modification|location_reference_point_id|front_tire|rear_tire|ride_year|ride_month|ride_day| id|\n",
      "+----------+------------------+------------------+-------------------+-------------------+-----------------+--------------------+--------------------+------------------+----------+-------------+--------------------------+---------------------------+----------+---------+---------+----------+--------+---+\n",
      "|2995939919|             -4500|-3.797957151394154| -3521.043060157007|-14369.165395960514|74.93882157175719|0.001124144578149...|-0.01609978051624...| 1.227161627220853|        55|            1|                         5|                          0|         5|        5|     2022|         5|      26| 99|\n",
      "|3035705320|             -4500|-3.797957151394154|-3521.0430574333186| -14369.16538973904|74.93882175203225|0.001122510806725...|-0.01610128932601...| 1.227161714444081|        55|            1|                         5|                          0|         5|        5|     2022|         5|      26| 99|\n",
      "|3075283052|             -4500|-3.797957151394154| -3521.043057781882|-14369.165390721595|74.93882173528742|0.001122472039581639|-0.01610144580303...|1.2271617852242378|        55|            1|                         5|                          0|         5|        5|     2022|         5|      26| 99|\n",
      "|3115703771|             -4500|-3.797957151394154|-3521.0430578946857|-14369.165405105308|74.93880849023469|0.001138693716928...|-0.01610436367811351|1.2271605743084346|        55|            1|                         5|                          0|         5|        5|     2022|         5|      26| 99|\n",
      "|3156080999|             -4500|-3.797957151394154| -3521.043058115309|-14369.165407315675|74.93880698484828|0.001141067507525403|-0.01610499546494479|1.2271604446195956|        55|            1|                         5|                          0|         5|        5|     2022|         5|      26| 99|\n",
      "+----------+------------------+------------------+-------------------+-------------------+-----------------+--------------------+--------------------+------------------+----------+-------------+--------------------------+---------------------------+----------+---------+---------+----------+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ids = train_ids[:100]\n",
    "files = ['control.csv', 'localization.csv', 'metadata.json']\n",
    "data_all = make_sp_df_all_ids(data_train_path, ids, files)\n",
    "data_all.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_x = 'x'\n",
    "target_y = 'y'\n",
    "target_yaw = 'yaw'\n",
    "not_target = ['z', 'roll', 'pitch', 'id']\n",
    "\n",
    "X = data_all.drop(not_target, 'x', 'y', 'yaw')\n",
    "target_x = data_all.select('x')\n",
    "target_y = data_all.select('y')\n",
    "target_yaw = data_all.select('yaw')\n",
    "\n",
    "X_train, X_test, target_x_train, target_x_test = train_test_split(X, target_x, test_size=0.2, random_state=RAND_ST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_sp = spark.read.csv(f'{data_train_path}/{train_ids[0]}/{files_temp[0]}', header=True, inferSchema=True)\n",
    "localization_sp = spark.read.csv(f'{data_train_path}/{train_ids[0]}/{files_temp[1]}', header=True, inferSchema=True)\n",
    "metadata_sp = spark_read_metadata(data_train_path, train_ids, 0, files_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_max(control, localization):\n",
    "    '''Find min and max timestamp in localization for each timestamp in control dataframe'''\n",
    "    \n",
    "    # Join control with localization to find the closest timestamps\n",
    "    control_with_min_max = control.alias('control').join(\n",
    "        localization.alias('localization'),\n",
    "        on=F.col('localization.stamp_ns') >= F.col('control.stamp_ns'),\n",
    "        how='left'\n",
    "    ).withColumn(\n",
    "        'loc_stamp_max',\n",
    "        F.min('localization.stamp_ns').over(Window.partitionBy('control.stamp_ns'))\n",
    "    # ).join(\n",
    "    #     localization.alias('localization_min'),\n",
    "    #     on=F.col('localization_min.stamp_ns') < F.col('control.stamp_ns'),\n",
    "    #     how='left'\n",
    "    # ).withColumn(\n",
    "    #     'loc_stamp_min',\n",
    "    #     F.max('localization_min.stamp_ns').over(Window.partitionBy('control.stamp_ns'))\n",
    "    )\n",
    "\n",
    "    return control_with_min_max.select('control.*', 'loc_stamp_min', 'loc_stamp_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `loc_stamp_min` cannot be resolved. Did you mean one of the following? [`loc_stamp_max`, `control`.`stamp_ns`, `localization`.`x`, `localization`.`y`, `localization`.`z`].;\n'Project [stamp_ns#1420L, acceleration_level#1421, steering#1422, 'loc_stamp_min, loc_stamp_max#1684L]\n+- Project [stamp_ns#1420L, acceleration_level#1421, steering#1422, stamp_ns#1443L, x#1444, y#1445, z#1446, roll#1447, pitch#1448, yaw#1449, loc_stamp_max#1684L]\n   +- Project [stamp_ns#1420L, acceleration_level#1421, steering#1422, stamp_ns#1443L, x#1444, y#1445, z#1446, roll#1447, pitch#1448, yaw#1449, loc_stamp_max#1684L, loc_stamp_max#1684L]\n      +- Window [min(stamp_ns#1443L) windowspecdefinition(stamp_ns#1420L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS loc_stamp_max#1684L], [stamp_ns#1420L]\n         +- Project [stamp_ns#1420L, acceleration_level#1421, steering#1422, stamp_ns#1443L, x#1444, y#1445, z#1446, roll#1447, pitch#1448, yaw#1449]\n            +- Join LeftOuter, (stamp_ns#1443L >= stamp_ns#1420L)\n               :- SubqueryAlias control\n               :  +- Relation [stamp_ns#1420L,acceleration_level#1421,steering#1422] csv\n               +- SubqueryAlias localization\n                  +- Relation [stamp_ns#1443L,x#1444,y#1445,z#1446,roll#1447,pitch#1448,yaw#1449] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfind_min_max\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontrol_sp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocalization_sp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[21], line 21\u001b[0m, in \u001b[0;36mfind_min_max\u001b[0;34m(control, localization)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Join control with localization to find the closest timestamps\u001b[39;00m\n\u001b[1;32m      5\u001b[0m control_with_min_max \u001b[38;5;241m=\u001b[39m control\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontrol\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m      6\u001b[0m     localization\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalization\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      7\u001b[0m     on\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalization.stamp_ns\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontrol.stamp_ns\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     F.max('localization_min.stamp_ns').over(Window.partitionBy('control.stamp_ns'))\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontrol_with_min_max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontrol.*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloc_stamp_min\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloc_stamp_max\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:3036\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2992\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   2993\u001b[0m \n\u001b[1;32m   2994\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3034\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3036\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `loc_stamp_min` cannot be resolved. Did you mean one of the following? [`loc_stamp_max`, `control`.`stamp_ns`, `localization`.`x`, `localization`.`y`, `localization`.`z`].;\n'Project [stamp_ns#1420L, acceleration_level#1421, steering#1422, 'loc_stamp_min, loc_stamp_max#1684L]\n+- Project [stamp_ns#1420L, acceleration_level#1421, steering#1422, stamp_ns#1443L, x#1444, y#1445, z#1446, roll#1447, pitch#1448, yaw#1449, loc_stamp_max#1684L]\n   +- Project [stamp_ns#1420L, acceleration_level#1421, steering#1422, stamp_ns#1443L, x#1444, y#1445, z#1446, roll#1447, pitch#1448, yaw#1449, loc_stamp_max#1684L, loc_stamp_max#1684L]\n      +- Window [min(stamp_ns#1443L) windowspecdefinition(stamp_ns#1420L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS loc_stamp_max#1684L], [stamp_ns#1420L]\n         +- Project [stamp_ns#1420L, acceleration_level#1421, steering#1422, stamp_ns#1443L, x#1444, y#1445, z#1446, roll#1447, pitch#1448, yaw#1449]\n            +- Join LeftOuter, (stamp_ns#1443L >= stamp_ns#1420L)\n               :- SubqueryAlias control\n               :  +- Relation [stamp_ns#1420L,acceleration_level#1421,steering#1422] csv\n               +- SubqueryAlias localization\n                  +- Relation [stamp_ns#1443L,x#1444,y#1445,z#1446,roll#1447,pitch#1448,yaw#1449] csv\n"
     ]
    }
   ],
   "source": [
    "find_min_max(control_sp, localization_sp).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spark_df_model(control:SparkDataFrame, localization:SparkDataFrame, metadata:SparkDataFrame):\n",
    "\n",
    "    \n",
    "    def find_min_max(control, localization):\n",
    "        '''Find min and max timestamp in localization for each timestamp in control dataframe'''\n",
    "        \n",
    "        # Join control with localization to find the closest timestamps\n",
    "        control_with_min_max = control.alias('control').join(\n",
    "            localization.alias('localization'),\n",
    "            on=F.col('localization.stamp_ns') >= F.col('control.stamp_ns'),\n",
    "            how='left'\n",
    "        ).withColumn(\n",
    "            'loc_stamp_max',\n",
    "            F.min('localization.stamp_ns').over(Window.partitionBy('control.stamp_ns'))\n",
    "        ).join(\n",
    "            localization.alias('localization_min'),\n",
    "            on=F.col('localization_min.stamp_ns') < F.col('control.stamp_ns'),\n",
    "            how='left'\n",
    "        ).withColumn(\n",
    "            'loc_stamp_min',\n",
    "            F.max('localization_min.stamp_ns').over(Window.partitionBy('control.stamp_ns'))\n",
    "        )\n",
    "\n",
    "        return control_with_min_max.select('control.*', 'loc_stamp_min', 'loc_stamp_max')\n",
    "    \n",
    "    def merge_min_max(control_2m, localization):\n",
    "        '''Merge min and max timestamp in localization for each timestamp in control dataframe'''\n",
    "        \n",
    "        control_3m = control_2m.join(\n",
    "            localization.withColumnRenamed('stamp_ns', 'stamp_ns_max').alias('localization_max'),\n",
    "            on=control_2m['loc_stamp_max'] == F.col('localization_max.stamp_ns_max'),\n",
    "            how='left'\n",
    "        ).join(\n",
    "            localization.withColumnRenamed('stamp_ns', 'stamp_ns_min').alias('localization_min'),\n",
    "            on=control_2m['loc_stamp_min'] == F.col('localization_min.stamp_ns_min'),\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Rename columns for clarity\n",
    "        for col in ['x', 'y', 'z', 'roll', 'pitch', 'yaw']:\n",
    "            control_3m = control_3m.withColumnRenamed(f'localization_max.{col}', f'{col}_max')\n",
    "            control_3m = control_3m.withColumnRenamed(f'localization_min.{col}', f'{col}_min')\n",
    "\n",
    "        return control_3m.drop('loc_stamp_max', 'loc_stamp_min')\n",
    "    \n",
    "    def interpolate_coords(control_3m, col_min, col_max):\n",
    "        '''Interpolate values between max and min values'''\n",
    "        \n",
    "        interpolation_expr = (\n",
    "            (F.col('stamp_ns') - F.col('stamp_ns_min')) / (F.col('stamp_ns_max') - F.col('stamp_ns_min')) *\n",
    "            (F.col(col_max) - F.col(col_min)) + F.col(col_min)\n",
    "        )\n",
    "        \n",
    "        return control_3m.withColumn(col_min.split('_')[0], interpolation_expr)\n",
    "\n",
    "    control_2m = find_min_max(control, localization)\n",
    "    control_3m = merge_min_max(control_2m, localization)\n",
    "\n",
    "    # Interpolate each coordinate column\n",
    "    coords_cols = ['x', 'y', 'z', 'roll', 'pitch', 'yaw']\n",
    "    for col in coords_cols:\n",
    "        control_3m = interpolate_coords(control_3m, f'{col}_min', f'{col}_max')\n",
    "    \n",
    "    contr_cols = ['stamp_ns', 'acceleration_level', 'steering']\n",
    "    control_inter = control_3m.select(*contr_cols, *coords_cols)\n",
    "    \n",
    "    def tires_to_columns_date(metadata):\n",
    "        '''Change tires column to front and rear columns and convert ride_date to datetime and add year, month, day columns'''\n",
    "        \n",
    "        # Add columns for front and rear tires\n",
    "        metadata_with_tires = metadata.withColumn('front_tire', F.col('tires')[0]) \\\n",
    "                                      .withColumn('rear_tire', F.col('tires')[1]) \\\n",
    "                                      .drop('tires')\n",
    "        \n",
    "        # Convert ride_date to datetime and extract year, month, day\n",
    "        metadata_with_tires = metadata_with_tires.withColumn('ride_date', F.to_date(F.col('ride_date'))) \\\n",
    "                                                 .withColumn('ride_year', F.year(F.col('ride_date'))) \\\n",
    "                                                 .withColumn('ride_month', F.month(F.col('ride_date'))) \\\n",
    "                                                 .withColumn('ride_day', F.dayofmonth(F.col('ride_date'))) \\\n",
    "                                                 .drop('ride_date')\n",
    "        \n",
    "        return metadata_with_tires\n",
    "\n",
    "    def add_metadata(control, metadata):\n",
    "        '''Add metadata to each row in control dataframe'''\n",
    "        \n",
    "        metadata_row = metadata.first()  # Assuming only one row for metadata\n",
    "        for col in metadata.columns:\n",
    "            control = control.withColumn(col, F.lit(metadata_row[col]))\n",
    "        \n",
    "        return control\n",
    "\n",
    "    # Process metadata\n",
    "    metadata_m = tires_to_columns_date(metadata)\n",
    "    \n",
    "    # Add metadata to each row in the control DataFrame\n",
    "    control_model = add_metadata(control_inter, metadata_m)\n",
    "\n",
    "    return control_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_sp = spark.read.csv(f'{data_train_path}/{train_ids[0]}/{files_temp[0]}', header=True, inferSchema=True)\n",
    "localization_sp = spark.read.csv(f'{data_train_path}/{train_ids[0]}/{files_temp[1]}', header=True, inferSchema=True)\n",
    "metadata_sp = spark_read_metadata(data_train_path, train_ids, 0, files_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['control.csv', 'localization.csv', 'metadata.json']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data in each file\n",
    "def read_data_pandas(path: str, ids: pd.Series, files: list):\n",
    "    '''Read data in each file'''\n",
    "    for i in ids:\n",
    "        data = []\n",
    "        for file in files:\n",
    "            if file == 'control.csv':\n",
    "                data.append(pd.read_csv(f'{path}/{i}/{file}'))\n",
    "            elif file.endswith('.json'):\n",
    "                data.append(json.load(open(f'{path}/{i}/{file}')))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data in each file with spark\n",
    "def read_data_spark(path: str, ids: pd.Series, files: list):\n",
    "    '''Read data in each file with spark'''\n",
    "    for i in ids:\n",
    "        data = []\n",
    "        for file in files:\n",
    "            if file == 'control.csv':\n",
    "                conrtol = spark.read.csv(f'{path}/{i}/{file}', header=True, inferSchema=True)\n",
    "            elif file == 'localization.csv':\n",
    "                localization = spark.read.csv(f'{path}/{i}/{file}', header=True, inferSchema=True)\n",
    "            elif file == 'metadata.json':\n",
    "                metadata = spark.read.json(f'{path}/{i}/{file}', multiLine=True, mode='PERMISSIVE'))\n",
    "   \n",
    "    \n",
    "    return "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
